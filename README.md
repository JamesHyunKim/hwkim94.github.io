# Github Blog
- 공부한 내용을 정리하여 포스팅하는 personal archive로 사용하고 있습니다.
- 혹시 잘못된 내용이나, 문의가 있으시면 블로그에 나와있는 주소로 연락주세요!

<table>
  <tr>
      <th colspan="4">Paper</th>
  </tr>
  <tr>
    <th>Num</th>
    <th>Title</th>
    <th>Review</th> 
    <th>Paper</th>
  </tr>

  <tr>
    <td>1</td>
    <td>resNet[1] Deep Residual Learning for Image Recognition(2015)</td>
    <td><a href="https://hwkim94.github.io/deeplearning/cnn/resnet/paperreview/2018/02/10/resNet1.html">review</a></td>
    <td><a href="https://arxiv.org/abs/1512.03385">paper</a></td>
  </tr>

  <tr>
    <td>2</td>
    <td>resNet[2] Identity Mappings in Deep Residual Networks(2016)</td>
    <td><a href="https://hwkim94.github.io/deeplearning/cnn/resnet/paperreview/2018/02/11/resNet2.html">review</a></td>
    <td><a href="https://arxiv.org/abs/1603.05027">paper</a></td>
  </tr>

  <tr>
    <td>3</td>
    <td>resNet[3] Residual Networks Behave Like Ensembles of Relatively Shallow Networks(2016)</td>
    <td><a href="https://hwkim94.github.io/deeplearning/cnn/resnet/paperreview/2018/02/11/resNet3.html">review</a></td>
    <td><a href="https://arxiv.org/abs/1605.06431">paper</a></td>
  </tr>

  <tr>
    <td>4</td>
    <td>VDCNN[1] Very Deep Convolutional Networks for Text Classification(2016) </td>
    <td><a href="https://hwkim94.github.io/deeplearning/cnn/nlp/paperreview/2018/02/17/VDCNN1.html">review</a></td>
    <td><a href="https://arxiv.org/abs/1606.01781">paper</a></td>
  </tr>

  <tr>
    <td>5</td>
    <td>CoVe[1] Learned in Translation: Contextualized Word Vectors(2017)</td>
    <td><a href="https://hwkim94.github.io/deeplearning/rnn/nlp/paperreview/2018/02/17/CoVe1.html">review</a></td>
    <td><a href="https://arxiv.org/abs/1708.001075">paper</a></td>
  </tr>

  <tr>
    <td>6</td>
    <td>wordCNN[1] Convolutional Neural Networks for Sentence Classification(2014)</td>
    <td><a href="https://hwkim94.github.io/deeplearning/cnn/nlp/paperreview/2018/02/19/wordCNN1.html">review</a></td>
    <td><a href="https://arxiv.org/abs/1408.5882">paper</a></td>
  </tr>

  <tr>
    <td>7</td>
    <td>wordCNN[2] Sensitivity Analysis of CNN sentence classification(2015) - Review(2015)</td>
    <td><a href="https://hwkim94.github.io/deeplearning/cnn/nlp/paperreview/2018/02/19/wordCNN2.html">review</a></td>
    <td><a href="https://arxiv.org/abs/1510.03820">paper</a></td>
  </tr>

  <tr>
    <td>8</td>
    <td>charCNN[1] Character level Convolutional Networks for Text Classification(2015)</td>
    <td><a href="https://hwkim94.github.io/deeplearning/cnn/nlp/paperreview/2018/02/20/charCNN1.html">review</a></td>
    <td><a href="https://arxiv.org/abs/1509.01626">paper</a></td>
  </tr>

  <tr>
    <td>9</td>
    <td>LSTM[1] Long Short Term Memory Recurrent Neural Network Architectures for Large Scale Acoustic Modeling(2014)</td>
    <td><a href="https://hwkim94.github.io/deeplearning/rnn/lstm/nlp/paperreview/2018/02/21/LSTM1.html">review</a></td>
    <td><a href="http://www.isca-speech.org/archive/archive_papers/interspeech_2014/i14_0338.pdf">paper</a></td>
  </tr>
  
  <tr>
    <td>10</td>
    <td>BLSTM[1] Bi-directional LSTM Recurrent Neural Network for Chinese Word Segmentation(2016)</td>
    <td><a href="https://hwkim94.github.io/deeplearning/rnn/lstm/nlp/paperreview/2018/02/21/BLSTM1.html">review</a></td>
    <td><a href="https://arxiv.org/abs/1602.04874">paper</a></td>
  </tr>

  <tr>
    <td>11</td>
    <td>DBLSTM[1] Hybrid Speech Recognition With Deep Bidirectional LSTM(2013)</td>
    <td><a href="https://hwkim94.github.io/deeplearning/rnn/lstm/nlp/paperreview/2018/02/21/DBLSTM1.html">review</a></td>
    <td><a href="https://www.cs.toronto.edu/~graves/asru_2013.pdf">paper</a></td>
  </tr>

  <tr>
    <td>12</td>
    <td>Attention[1] Neural Machine Translation by Jointly Learning to Align and Translate(2014)</td>
    <td><a href="https://hwkim94.github.io/deeplearning/rnn/lstm/attention/nlp/paperreview/2018/02/23/attention1.html">review</a></td>
    <td><a href="https://arxiv.org/abs/1409.0473">paper</a></td>
  </tr>

  <tr>
    <td>13</td>
    <td>Attention[2] Effective Approaches to Attention based Neural Machine Translation(2015)</td>
    <td><a href="https://hwkim94.github.io/deeplearning/rnn/lstm/attention/nlp/paperreview/2018/02/23/attention2.html">review</a></td>
    <td><a href="http://aclweb.org/anthology/D15-1166">paper</a></td>
  </tr>

  <tr>
    <td>14</td>
    <td>BiDAF[1] Editing Bidirectional Attention Flow for Machine Comprehension(2016)</td>
    <td><a href="https://hwkim94.github.io/deeplearning/rnn/lstm/attention/nlp/paperreview/2018/02/23/BiDAF1.html">review</a></td>
    <td><a href="https://arxiv.org/abs/1611.01603">paper</a></td>
  </tr>
  
  <tr>
    <td>15</td>
    <td>Seq2Seq[1] Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation(2014)</td>
    <td><a href="https://hwkim94.github.io/deeplearning/rnn/seq2seq/nlp/paperreview/2018/02/24/seq2seq1.html">review</a></td>
    <td><a href="https://arxiv.org/abs/1406.1078">paper</a></td>
  </tr>

  <tr>
    <td>16</td>
    <td>Seq2Seq[2] Sequence to Sequence Learning with Neural Networks(2014)</td>
    <td><a href="https://hwkim94.github.io/deeplearning/rnn/lstm/seq2seq/nlp/paperreview/2018/02/25/seq2seq2.html">review</a></td>
    <td><a href="https://arxiv.org/abs/1409.3215">paper</a></td>
  </tr>
  
  <tr>
    <td>17</td>
    <td>GRU[1] Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling(2014)</td>
    <td><a href="https://hwkim94.github.io/deeplearning/rnn/gru/paperreview/2018/02/27/GRU1.html">review</a></td>
    <td><a href="https://arxiv.org/abs/1412.3555">paper</a></td>
  </tr>
  
  <tr>
    <td>18</td>
    <td>char-word CNN[1] Character Word LSTM Language Models(2017)</td>
    <td><a href="https://hwkim94.github.io/deeplearning/cnn/charcnn/wordcnn/nlp/paperreview/2018/02/27/char-wordCNN1.html">review</a></td>
    <td><a href="https://arxiv.org/abs/1704.02813">paper</a></td>
  </tr>
  
</table>


<table>
  <tr>
      <th colspan="4">Implementation</th>
  </tr>
  <tr>
    <th>Num</th>
    <th>Title</th>
    <th>Review</th> 
    <th>Code</th>
  </tr>

  <tr>
    <td>1</td>
    <td>resNet[4]</td>
    <td><a href="https://hwkim94.github.io/deeplearning/cnn/resnet/implementation/tensorflow/2018/02/21/resNet4.html">review</a></td>
    <td><a href="https://github.com/hwkim94/hwkim94.github.io/tree/master/Implementation/resNet">code</a></td>
  </tr>
</table>
