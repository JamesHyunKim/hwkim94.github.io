---
layout: post
title:  "resNet[1] Deep Residual Learning for Image Recognition(2015) - Review"
date:   2018-02-10 15:37:00 +0900
categories: [cnn, deeplearning, paperreview]
---

## Abstract
- residual : 남은, 나머지의, 잔여의
- model을 더 깊게 만드는 것이 목적
- resNet을 통하여 layer가 깊어질수록 성능이 좋아짐

<p></p>

-----

## Introduction
- network depth is crucial importance, 하지만 layer가 늘어날 수록 문제가 발생
- vanishing/exploding gradient문제가 발생
    - normalized initialization과 intermediate normalization layers(batch normalization)로 해결
    
<p></p>

- degradation problem 문제 발생
    - ![problem](https://files.slack.com/files-pri/T1J7SCHU7-F96D95J13/problem.png?pub_secret=12c276bad3)
    - depth가 깊어져도 더 이상 error가 줄어들지 않고 오히려 늘어남
    - 이 문제를 해결하기 위해 residual learning을 제시
    
<p></p>

- residual learning
    - layer를 그냥 쌓는 것이 아니라 residual mapping을 통하여 쌓음
    - residual mapping = $$F(x) + x$$을 의미

# residual block
![residual block](https://files.slack.com/files-pri/T1J7SCHU7-F967PC3L4/rb.png?pub_secret=d4fd4fb528)

- shortcut connection(path)과 main connection(path)으로 나누어짐
    - shortcut connection : 1개 이상의 layer를 뛰어 넘는 것, 즉 identity mapping
    - main connection : 그대로 layer를 통과
    
<p></p>

- 두 path의 결과를 더해줘서 activation function에 넣어줌
- 이렇게 쌓은 모델이 optimize하기 더 쉽고, 더 깊은 모델을 만들어 성능이 좋아짐

-----

## Related Work
- Residual Representations
- Shortcut Connections

-----

## Deep Residual Learning
# Residual Learning
- multiple nonlinear layers가 complicated functions를 근사할 수 있다면, residual functions도 근사 가능
- 따라서 $$H(x) = F(x) + x$$를 구현하지 않고, residual function $$F(x) = H(x) - x$$를 구현 후 변환
    - F(x) = 0이 되는 것이 이상적이므로, 학습 방향이 정해져있음(pre-conditioning)
    
<p></p>

- identity mapping layer가 쌓이게 되면, 적어도 없는 것보다는 error가 커지지는 않음
    - 최소한 input 그대로를 output으로 내보냄
    
<p></p>

- 만약 학습과정에서 optimal function이 identity에 수렴하면, 어디가 문제인지 찾아낼 수 있음
    - 기존의 방식대로 학습한다면, 새로운 모델을 다시 학습시켜야하므로 보다 간편해짐 
    - identity와 수렴하는 이유 : shortcut path가 있으므로, layer가 0으로 수렴하면 identity가 됨

# Identity Mapping by Shortcuts
- input과 output의 차원이 같다면,
    - ![m1](https://files.slack.com/files-pri/T1J7SCHU7-F965XUR7V/1.png?pub_secret=b5bc4bf6ab)
    - 만약 2개의 layer를 skip한다면, $$y = {W_{1}} * ReLU({W_{2}} * x) + x$$
    
<p></p>

- input과 output의 차원이 다르다면, shortcut connection을 linear projextion
    - ![m2](https://files.slack.com/files-pri/T1J7SCHU7-F95K36W4Q/2.png?pub_secret=8d5e0a41c1)

- 여러 개의 layer를 skip하면서 shortcut path를 만들어봤지만, 오직 한 개의 layer만 skip할 경우에는 $$y = Wx + x$$ 형태의 linear layer가 되어버리므로 효과가 없음

<p></p>

- shortcut path는 parameter를 요구하지도 않고, computer complexity를 증가시키지도 않음

# Network Architecture
![architecture](https://files.slack.com/files-pri/T1J7SCHU7-F97B6L51V/architecture.png?pub_secret=8490108baf)
- plain network
    - feature map size가 같으면 같은 수의 filter 사용
    - feature map size가 절반이 되면, filter의 수를 2배로 사용
    
<p></p>

- residual network
    - 차원이 늘어날 경우는 남은자리에 zero-padding, 줄어들면 projection
    
# Implementation
- convolution직후, activation 직전에 Batch normalization
- 초기 값은 plain network에서 학습한 것을 사용

-----

## Experiment
![e1](https://files.slack.com/files-pri/T1J7SCHU7-F969UAG2E/e1.png?pub_secret=07ac906498)
- resNet은 깊어질 수록 에러가 줄어들었고, 다른 모델보다도 성능이 좋았다.
- resNet이 더 빠르게 converge
- deep plain network의 경우 convergence rate가 기하적으로 감소했을 것이라 추측
- 차원조절은 가능하면 하지 않는 것이 성능에 더 좋음
- 다양한 깊이의 resNet을 ensemble로 사용하면 더 효과가 좋음

# Bottleneck Architecture

![bottleneck](https://files.slack.com/files-pri/T1J7SCHU7-F973CB9QA/1.png?pub_secret=dbac453ecf)
- 학습시간을 고려하여 50개 이상의 layer를 쌓은 경우엔 구조 변경
- 차원을 줄였다가 다시 늘리는 방식
- 연산시간 감소 목적

-----

## Reference
- [https://arxiv.org/abs/1512.03385](https://arxiv.org/abs/1512.03385)
- [https://laonple.blog.me/220764986252](https://laonple.blog.me/220764986252)
