---
layout: post
title:  "resNet[2] Identity Mappings in Deep Residual Networks(2016) - Review"
date:   2018-02-11 05:38:00 +0900
categories: [deeplearning, cnn, resnet, paperreview]
---

## 1. Abstract
- [residual network](https://hwkim94.github.io/cnn/deeplearning/paperreview/2018/02/10/resNet1.html)가 왜 효과적인지, 어떻게 하면 더 개선할 수 있는지에 관한 논문
- residual block을 사용하면 vanishing gradient 문제가 해결됨을 수식으로 증명
    - 오차역을 어느 block으로든 잘 전달할 수 있게됨

-----

## 2. Introduction
- general form of residual unit
    - $${y_{l}}$$ = $$h({x_{l}}) + F({x_{l},{W_{l}}})$$
    - $${x_{l+1}}$$ = $$f({y_{l}})$$
    - $$h$$ = identity mapping, $$f$$ = ReLU, $$F$$ = residual function
- **shortcut path에는 아무것도 해주지 않는 것이 제일 효과적(proposed residual unit)**
    - ![proposed residual unit](https://files.slack.com/files-pri/T1J7SCHU7-F979T2QDB/1.png?pub_secret=fae8a5d0a6)
    - f를 identity mapping으로 만들어주기 위하여, 'pre-activataion' 방식으로 weight layer 재구성

-----

## 3. Analysis of Deep Residual Network
- proposed residual unit의 경우 $$f$$ = identity mapping을 의미
    - $${x_{l+1}}$$ = $$f({y_{l}})$$ = $${y_{l}}$$
- L > l 일 경우에 모든 $${x_{L}}$$은 $${x_{l}}$$로 표현 가능함
    - 따라서, chain rule 을 통하여 back propagation을 살펴 볼 수 있음
    - $${x_{l+1}}$$  = $${x_{l}} + F({x_{l},{W_{l}}})$$
    - $$x_{L}$$ = $$x_{l} + \sum_{i=l}^{L-1}F(x_{i}, W_{i})$$
    - $$cost$$ = $$\varepsilon$$
    - ![back propagation](https://files.slack.com/files-pri/T1J7SCHU7-F9881H5PH/2.png?pub_secret=f7b8140a30) 
    - 위의 식을 살펴보면, $$W$$가 아무리 작아도 $$L$$에서의 gradient가 $$l$$로 전달됨
    - vanishing gradient가 발생하지 않음

-----

## 4. On the Importance of Identity Skip Connections
- shortcut connection에 scalar를 곱해주는 경우
    - $$h(x) = \lambda * x$$ 가 됨
    - ![m1](https://files.slack.com/files-pri/T1J7SCHU7-F96GZEGD6/3.png?pub_secret=7a0282b83a)
    - ![m2](https://files.slack.com/files-pri/T1J7SCHU7-F97A8URDK/4.png?pub_secret=d5a7adb7e8) 
    - $$\lambda > 0 $$ 인 경우에는 값이 너무 커지고, $$\lambda < 0 $$ 값이 너무 작아짐
    - shortcut connection은 건드리지 않는 것이 좋음

- 다른 여러가지 architecture 시도
![attempt](https://files.slack.com/files-pri/T1J7SCHU7-F9767CQ12/other.png?pub_secret=1aac648e18)

# 4.1 Experiments on skip connection
- shortcut path는 clear하게 구성하는 것이 좋음
    - ![e1](https://files.slack.com/files-pri/T1J7SCHU7-F973VBV0B/e1.png?pub_secret=f6a79436ae)

-----

## 5. On the Usage of Activation Functions
- activation function과 normalization, convolution의 위치를 바꿔가며 실험
- activation을 취해주지 않아야 제대로 propagated 됨
- **BN과 ReLU를 함께 사용하는 것이 성능이 좋음**
    - training error는 기존 모델보다 높지만, 일반화가 잘 되어서 test error는 낮음

# 5.1 Experiments on Activation
- activation 및 normalization의 경우도 convolution layer에서만 처리해주는 것이 좋음
    - ![e2](https://files.slack.com/files-pri/T1J7SCHU7-F973VBV0B/e1.png?pub_secret=f6a79436ae)

-----

## 6. Reference
- [https://arxiv.org/abs/1603.05027](https://arxiv.org/abs/1603.05027)
- [https://laonple.blog.me/220793640991](https://laonple.blog.me/220793640991)
- [https://kangbk0120.github.io/articles/2018-01/identity-mapping-in-deep-resnet](https://kangbk0120.github.io/articles/2018-01/identity-mapping-in-deep-resnet)
- Ybigta deepNLP-study
