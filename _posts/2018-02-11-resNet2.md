---
layout: post
title:  "resNet[2] Identity Mappings in Deep Residual Networks(2016) - Review"
date:   2018-02-11 04:08:00 +0900
categories: [cnn, deeplearning, paperreview]
---

# Abstract
- residual network가 왜 효과적인지, 어떻게 하면 더 개선할 수 있는지에 관한 논문
- residual block을 사용하면 vanishing gradient 문제가 해결됨을 수식으로 증명
    - 오차역을 어느 block으로든 잘 전달할 수 있게됨

-----

# Introduction
- general form of residual unit
    - $${y_{l}} = h({x_{l}}) + F({x_{l},{W_{l}}})$$
    - $${x_{l+1}} = f({y_{l}})$$
    - h = identity mapping, f = ReLU
- shortcut path에는 아무것도 해주지 않는 것이 제일 효과적(proposed residual unit)
    - ![proposed residual unit](https://files.slack.com/files-pri/T1J7SCHU7-F979T2QDB/1.png?pub_secret=fae8a5d0a6)
    - f를 identity mapping으로 만들어주기 위하여, 'pre-activataion' 방식으로 weight layer 재구성

# Analysis of Deep Residual Network
- proposed residual unit의 경우 f = identity mapping을 의미
    - $${x_{l+1}} = f({y_{l}}) = {y_{l}}$$
- L > l 일 경우에 모든 $${x_{L}}$$은 $${x_{l}}$$로 표현 가능함
    - 따라서, chain rule 을 통하여 back propagation을 살펴 볼 수 있음
    - $${x_{l+1}}  = {x_{l}} + F({x_{l},{W_{l}}})$$
    - $$x_L = x_l + \sum_{i=l}^{L-1}F(x_i, W_i)$$
    - $$cost = \varepsilon$$
    - ![back propagation](https://files.slack.com/files-pri/T1J7SCHU7-F9881H5PH/2.png?pub_secret=f7b8140a30) 
    - 위의 식을 살펴보면, W가 아무리 작아도 L에서의 gradient가 l로 전달됨
    - vanishing gradient가 발생하지 않음


# Reference
- [https://arxiv.org/abs/1603.05027](https://arxiv.org/abs/1603.05027)
- [https://laonple.blog.me/220793640991](https://laonple.blog.me/220793640991)
- [https://kangbk0120.github.io/articles/2018-01/identity-mapping-in-deep-resnet](https://kangbk0120.github.io/articles/2018-01/identity-mapping-in-deep-resnet)
