---
layout: post
title:  "VDCNN[1] Very Deep Convolutional Networks for Text Classification(2016) - Review"
date:   2018-02-17 03:23:00 +0900
categories: [deeplearning, cnn, charcnn, resnet, nlp, paperreview]
---

## 1. Abstract
- character level의 연산
- 적은 convolution과 pooling 사용으로 메모리 절약
- resNet을 활용하여 모델을 깊게 만들어 성능의 향상(29 convolution layers)

-----

## 2. Introduction
- convNet은 원래 이미지의 특징을 뽑아내고 분류하기 위하여 사용되는 것이지만, NLP에도 적용
- image 처리 분야에서 좋은 모델의 경우 layer의 수가 계속 증가하는 추세, 이것을 NLP에 적용

-----

## 3. Related Work
- cbow, n-gram, TF-IDF
- RNN
- CNN

-----

## 4. VDCNN architecture
# 4.1 architecture
![architecture](https://files.slack.com/files-pri/T1J7SCHU7-F8TJSSEKD/_______-1.png?pub_secret=70f9720b8e)

- **resNet과 VGG를 기반으로 모델링**
    - optional shortcut : resNet의 shortcut path방식, 기존의 6-layers에서 29-layers까지 발전
    - filter : VGG의 3-size filter을 응용하여 3-gram 역할, character의 특성을 반영하기 위해 size를 키우지 않음
    - 메모리 사용을 줄이는 효과

- look-up-table 
    - input layer 역할, 2D tensor
    - tensor size = embeddings of the $$s$$ characters = $$({f_{0}}, s)$$ 
    - $${f_{0}}$$ = dimension of input text, $$s$$ = 1024 fixed
    - character-level로 input을 받음

- 마지막에 k-max pooling을 통하여 $$512 \times k$$개의 feature를 뽑아낸 후, 벡터로 만들어 fully connected ReLU classifier로 분류
- regularization을 위하여 temporal batch norm만 사용했으며, dropout은 사용하지 않음

# 4.2 convolutional block
![convoulutional block](https://files.slack.com/files-pri/T1J7SCHU7-F8TR77329/_______-2.png?pub_secret=b2ef5ebf73)

- Convolution -> Batch Normalization -> ReLu 형태의 sequence of convolutional layers
- Temporal batch normalization 는 batch normalization과 비슷한 regularization 사용 
    - activation이 mini-batch일 경우 jointly normalized over temporal (instead of spatial) locations
- filter의 크기가 작아 parameter가 적음 
    - convolution layer를 통하여 network의 depth를 많이 늘리는 것이 가능
- 전체적인 architecture에서 depth 조절을 convolutional block 개수를 통해 조절

-----

## 5. Experimental evaluation & result
# 5.1 pooling 비교
![result1](https://files.slack.com/files-pri/T1J7SCHU7-F8TMBQY5S/_______-3.png?pub_secret=492d56be4e)
- character embedding size 16, mini-batch size 128, learning rate 0.01, momintum 0.9로 학습
- 중간중간 해주는 pooling에서 max-pooling이 다른 pooling기법보다 효과가 좋았다.

# 5.2 residual network 비교
![result2](https://files.slack.com/files-pri/T1J7SCHU7-F8TMBRQLU/_______-4.png?pub_secret=69f667839a)
- resNet을 적용한 것이 적용하지 않은 것보다 더 결과가 좋다.
- resNet을 통하여 모델이 깊어질 때 accuracy degradation문제를 해결 할 수 있다.
- layer가 29개일 때 가장 성능이 좋았다.

-----

## 6. Conclusion
- **NLP에 CNN을 적용할 때에도, resNet을 활용해 더 깊은 모델을 만드는 것이 성능에 좋다.**

-----

## 7.Reference
- [https://arxiv.org/abs/1606.01781](https://arxiv.org/abs/1606.01781)
- Ybigta deepNLP-study
