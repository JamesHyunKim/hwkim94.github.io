---
layout: post
title:  "wordCNN[2] Sensitivity Analysis of CNN sentence classification(2015) - Review"
date:   2018-02-19 09:01:00 +0900
categories: [deeplearning, cnn, nlp, paperreview]
---

## 1. Abstract
one-layer CNN의 성능을 Sensitivity Analysis에 실험하며, 좋은 성능을 보여주는 setting을 개략적으로 제시

-----

## 2. Introduction
- CNN은 distributed representations of words로 표현된 문장을 capitalize할 수 있다.
    - 'capitalize'는 sentence의 정보를 vector로 잘 표현 할 수 있다는 것을 의미하는 것 같다.
- CNN은 one-layer만으로도 성능이 좋다.
- 하지만 parameter setting,  architecture setting이 어렵다는 단점이 있다.

-----

## 3. Background and Preliminaries
- word embedding
- CNN

-----

## 4. Dataset
![dataset](https://files.slack.com/files-pri/T1J7SCHU7-F9B0AU0KE/d2.png?pub_secret=c5cf746dcb)

-----

## 5. Baseline Models
![model](https://files.slack.com/files-pri/T1J7SCHU7-F9ATDRA67/m1.png?pub_secret=b491520b54)
- 여러가지 조건을 변경하며 어떤 것이 제일 성능이 좋은지 실험
- one-layer이기 때문에 복잡하지 않은 architecture

-----

## 6. Conclusion
- input vector로 word2vec이나 GloVe를 사용하는 것이 one-hot보다 성능이 좋다.
- filter region(window size)도 성능에 큰 영향을 미친다.
    - 1~10이 가장 성능이 좋지만, 문장의 길이가 길어지면 좀 더 늘리는 것이 좋다.
    - 문장의 길이에 따라 달라진다.
- feature map의 수도 성능에 큰 영향을 미친다.
    - 100~600이 성능에 좋다.
- 1-max-pooling이 성능이 가장 좋다.
- regularization은 큰 영향을 미치지 않는다.

-----

## 7. Reference
- [https://arxiv.org/abs/1510.03820](https://arxiv.org/abs/1510.03820)
- Ybigta deepNLP-study
