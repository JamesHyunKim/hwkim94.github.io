---
layout: post
title:  "charCNN[1] Character level Convolutional Networks for Text Classification(2015) - Review"
date:   2018-02-20 07:01:00 +0900
categories: [cnn, deeplearning, paperreview, nlp]
---

## 1. Abstract
bag of words, n-grams TF-IDF variants, word-based CNN & RNN과 비교하여 charCNN의 성능을 실험적으로 탐구한 결과를 보여주는 논문

-----

## 2. Introduction
- Text Clssification 분야에서 word-based n-gram 모델이 좋은 성과를 보여주고 있다.
- CNN은 raw signal에서 특성을 추출하는데 탁원한 성능을 보인다.
- 둘을 응용하여 charCNN model을 만들었다.

-----

## 3. Character-level Convolutional Networks
# 3.1 Key Moduele
- max-pooling, ReLU, SGD 사용

# 3.2 Character quantization
- quantize = encoding의 의미로 사용됨
- $$m$$ = alphabet 개수(숫자, 특수문자 포함)
- one-hot encoding(1-of-m encoding) 사용
- character sequence의 길이를 $${l_{0}}$$로 고정
    - 이 길이를 초과하면 버리고, 부족하면 zero-vector를 채워줌
- character quantization order는 backward로 해서 최근에 읽힌 character는 output의 시작부분에 위치하도록 함 

# 3.3 Model Design
![architecture](https://files.slack.com/files-pri/T1J7SCHU7-F9BJ9QMU5/m1.png?pub_secret=79f27ad328)
![setting1](https://files.slack.com/files-pri/T1J7SCHU7-F9BE62160/m4.png?pub_secret=6939eb6ae3)
![setting2](https://files.slack.com/files-pri/T1J7SCHU7-F9CGBQDP1/m5.png?pub_secret=76b9f0cba3)
- parameter 수를 기준으로 large, small model을 각각 구현
- 6개의 conv layer, 3개의 fc layer
- 정규화를 위하여 p=0.5 dropout 사용

# 3.4 Data Augmentation using Thesaurus
- Augmentation = 확대, Thesaurus = 유의어 사전
- appropriate data augmentation은 딥러닝에서 generalization 문제를 어느정도 해결해준다.
- 언어는 순서가 있기 때문에 image같이 signal을 변경해주는 것은 불가하다.
- 문장의 구성을 바꾸는 등 rephrase하는 것은 불가능하므로, 유의어를 바꿔주는 방식 채택
- 얼마나 바꿀 것이고, 어떤 것으로 바꿀 것인지는 0.5확률의 geometric distribution 사용

-----

## 4. Comparision Model
# 4.1 Traditional Methods
- Bag-of-words and its TF-IDF
    - 50,000 most frequent words from the training subset
    - 빈도수와 TF-IDF값을 feature로 지정
- Bag-of-n-grams and its TF-IDF
    - 500,000 most frequent n-grams (up to 5-grams) from the training subset

- Bag-of-means on word embedding
    - K-means clustering을 word2vec embedding space에 적용
    - 같은 cluster 안에 있는 데이터들의 embedding의 평균을 사용

# 4.2 Deep Learning Methods
- word-based CNN
    - 300 size의 word2vec embedding 사용
- word-based LSTM
    - ![lstm](https://files.slack.com/files-pri/T1J7SCHU7-F9C9XS7E2/lstm1.png?pub_secret=3dc0720a29)
    - 300 size의 word2vec embedding 사용
    - LSTM cell들의 output값들의 평균을 feature로 사용하여 multinomial logistic regression에 사용
    - vanilla architecture 사용

# 4.3 Choice of Alphabet
대소문자를 구별하는 경우, 대부분 성능이 좋지 않았다. 그 이유는 같은 alphabet이라도 대소문자를 구분하여 다른 문자로 인식하기 때문에 regularization문제가 발생하는 것으로 추정된다.

-----

## 5. Large-scale Datasets and Results
![result](https://files.slack.com/files-pri/T1J7SCHU7-F9AT2S4GH/e1.png?pub_secret=a3144e5d14)
- AG : AG’s news corpus
- Sogou : Sogou news corpus
- DBP : DBPedia ontology dataset
- Yelp.P : Yelp reviews
    - predicting a polarity label by considering stars 1~2 negative, 3~4 positive
- Yelp.F : Yelp reviews
    - predicting full number of stars the user has given
- Yah.A : Yahoo! Answers dataset
- Amz.F : Amazon reviews
    - full score prediction
- Amz.P : Amazon reviews
    - polarity prediction

-----

## 6. Discussion
![result](https://files.slack.com/files-pri/T1J7SCHU7-F9BB42LF5/r1.png?pub_secret=e5f43cb498)
- Character-level ConvNet도 효율적인 모델이다.
- n-grams TFIDF은 dataset을 조금 늘릴 경우에 더 효과적이지만, data set을 많이 늘릴 경우에는 charCNN이 더 성능이 좋다.
- **charCNN이 리뷰, 댓글 등 User-generated data(Yelp, Yah, Amz)에 더 강하다.**
    - character 단위로 분석을 하기 때문에 맞춤법이 틀린 문자에 더 robust하다.
    - **반면에 맞춤법이  정확한 data(AG, Sogus, DBP)에서는 n-gram TF-IDF가 성능이 좋다.**
- data set이 커졌을 때는, 대소문자 구분한 것이 성능이 더 좋지 않았다.
    - regularization effect 문제로 추정

-----

## 7. Conclusion
character-level ConvNet is an effective method

-----

## 8. Reference
- [https://arxiv.org/abs/1509.01626](https://arxiv.org/abs/1509.01626)
- [https://godongyoung.github.io/](https://godongyoung.github.io/)
- Ybigta deepNLP-study
