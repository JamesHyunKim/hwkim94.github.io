---
layout: post
title:  "Attention[3] Editing Bidirectional Attention Flow for Machine Comprehension(2016) - Review"
date:   2018-02-23 08:00:00 +0900
categories: [rnn, deeplearning, paperreview, nlp, attention]
---

## 1. Abstract
- MC(Machine Comprehension) : answering a query about a given context paragraph
    - **문장(context)의 흐름을 이해하고, 주어진 문장(context)에서 질문(query)에 대답하는 것**
    - query와 context의 복잡한 상호관계를 이해해야함
    - ex) Context : 현우는 동영이에게 1000원을 줬다. 동영이는 우정이에게 다시 1000원을 줬다. 
    - ex) Query :  동영이가 가진 돈은?

- 최근에 Attention 모델이 MC에 사용되기 시작함
    - 문맥의 작은 부분에 집중 가능하며, 그 정보를 fixed-size vector로 표현

- BiDAF : Bi-Directional Attention Flow
    - multi-stage hierarchical process
    - context의 단어마다 가지는 맥락이 다를 것, 이것을 구분
    - context 요약 없이 query-aware context를 represent
- query-aware context
    - 질문과 관련된 context
    - query의 word들이 각각 context의 word와 연관되어 있는지 찾는 것
    - 따라서, context를 이해하고 answer 가능

-----

## 2. Introduction
- Attention mechanism은 target의 context paragraph를 잘 이해하게 해준다.
- Attention mechanism이 과거에 가졌던 특징
    - context를 fixed-size vector로 요약하여 attention weight 계산(문맥정보를 추출)
    - attention vector는 계속 update
    - uni-directional

- BiDAF
    - hierarchical multi-stage architecture
    - context의 서로 다른 수준의 세분성을 represent
    - char/word/contextual embedding 사용

# BiDAF의 장점
1. **attention layer가 문맥을 fixed-size vector로 요약하는 것으로 사용되지 않음.**
    - 과거의 모델은 data iteration으로 학습했지만, BiDAF는 이전 time step의 attention layers를 사용
    - 이전 layer에서 계산된 attended vector를 subsequent model로 flow하게 해줌
    - 이 방법을 통해 요약으로 인한 정보손실이 줄어들게 됨
2. memory-less attention
    - attention 학습에 이전 attention layers를 사용하므로 효율적
    - 따라서 그 자원을 query와 context의 attention 학습, 모델이 query-aware context 학습하는 것에 더 집중할 수 있음. 
    - 또한, 이전의 잘못된 attention의 영향을 받지 않게 됨

-----

## 3. Model
![model](https://files.slack.com/files-pri/T1J7SCHU7-F9DKE1BK9/m1.png?pub_secret=6b08024f7b)
- 6개의 layer로 이루어진 hierarchical multi-stage process

# 3.1 Character Embedding Layer 
- charCNN을 이용하여 word를 vector로 만들어줌
- character에 ID vector를 부여하고 CNN에 넣어서 word를 예측하는 방식으로 학습.
- CNN output을 pooling으로 차원을 줄여 embedding으로 사용

# 3.2 Word Embedding Layer 
- pre-trained word embedding model러 word를 vector로 만들어줌
- 논문에서는 GloVe 사용

# 3.3 Contextual Embedding Layer 
- char embedding과 word embedding을 concatenate한 후 2 layer highway network를 통과시킴
- word 주변의 contextual cues(문맥적 단서)를 사용하여 단어의 embedding을 재정의 
- BLSTM을 통해 주변 문맥을 파악할 수 있게 함
- $$H$$ = Context의 context matrix, $$U$$ = Query의 context matrix

# 3.4 Attention Flow Layer
- query와 context vector를 연결시키고, 각 단어마다 문맥 속의 query-aware feature vectors를 만듦
- 이전과는 다르게, query와 context를 single feature vectors로 요약하는 것에 사용되지 않음
- 각 time step의 attention vectors가 subsequent modeling layer로 들어가게 해주도록 함 
- 이를 통하여 요약을 통한 정보손실을 방지할 수 있음
- query-aware vector representations of the context words $$G$$를 output
- bi-direction으로 attention계산 : Context2Query, Query2Context
- similarity matrix $${S_{tj}}$$ = $$\alpha ({H_{:t}},{U_{:j}})$$ 공유
    - $${S_{tj}}$$ 는 t-th context word와 j-th query word의 similarity 의미
    - $$\alpha (h, u)$$ = $${w_{S}^{T}} [h; u; h \odot u]$$
    - $$\alpha$$ = a trainable scalar function, similarity를 encoding하는 역할
    - $${w_{S}^{T}}$$ = trainable weight vector
         
1. **Context2Query Attention**
    - 어떤 qeury word가 context word와 가장 관계가 있는지 signify
    - $${a_{t}}$$ = query word와 t-th context word의 attention weight 
    - $${a_{t}}$$ = $$softmax({S_{t:}})$$
    - 단, $$\sum {a_{tj}} = 1$$ for all t
    - attended query vector $$\widetilde{U_{:t}}$$ = $$\sum {a_{tj}}{U_{:j}}$$
    - 따라서, $$\widetilde{U}$$는 context 전체의 attended query vector를 담고 있음

2. **Query2Context Attention**
    - context word가 어떤 query word와 가장 관련 있는지 signify 
    - 따라서, query에 대답하는 것에서 중요한 역할
    - $${b}$$ = $$softmax({max_{col}}(S))$$
    - attened context vector $$\widetilde{h}$$ = $$\sum {b_{t}}{H_{:t}}$$
    - $$\widetilde{h}$$ = query를 고려했을 때, context에서 가장 중요한 vector의 가중합
    
- **query-aware representation of each context word**
    - $${G_{:t}}$$ = $$\beta ({H_{:t}}, \widetilde{U_{:t}}, \widetilde{H_{:t}})$$
    - $${G_{:t}}$$ = t-th context vector 
    - $$\beta(h, \widetilde{u}, \widetilde{h})$$ = $$[h;\widetilde{u};h \odot \widetilde{u};h \odot \widetilde{h}]$$

# 3.5 Modeling Layer  
- RNN을 통하여 context scan
- 위에서 구한 $$G$$가 input
- query와 context word의 interaction을 파악
- BLSTM 사용

# 3.6 Output Layer
- query에 대한 answer을 ouput
- 질문에 대한 대답으로 paragraph의 sub-phrase를 찾아야함 
- paragraph안에 있는 start, end의 indicies 를 predict 해서 phrase를 찾음
- $${p_{1}}$$ = $$softmax({w_{p_{1}}^{T}}[G;{M^{1}}])$$
- $${p_{2}}$$ = $$softmax({w_{p_{2}}^{T}}[G;{M^{2}}])$$
- $${M^{1}}, {M^{2}}$$는 각각 LSTM의 forward, backward output

# 3.7 Training
- loss : sum of negative pobalites of true start and end indices s by the predicted distributions
     - $$L( \theta ) = -\sum log({p_{y_{i}^{1}}^{1}}) + log({p_{y_{i}^{2}}^{2}})$$
     - $$\theta$$ = training weights
     - $${P_{k}}$$ = k-th value of vector P
     
# 3.8 Test
- The answer span $$(k, l)$$ with maximum value of $${P_{k}^{1}}{P_{l}^{1}}$$

-----

## 4. Related work
- Machine comprehension
- Visual question answering

-----

## 5. Question Answering Experiment
![result1](https://files.slack.com/files-pri/T1J7SCHU7-F9CH8EC9J/r1.png?pub_secret=511e9eb39c)
- Dataset : SQuAD
- Ensemble로 사용한 것이 제일 성능이 좋았다.

# 5.1 Visualization
![result2](https://files.slack.com/files-pri/T1J7SCHU7-F9CMDH6LD/r2.png?pub_secret=a327e1776b)
![result3](https://files.slack.com/files-pri/T1J7SCHU7-F9C1NCXT3/r3.png?pub_secret=f63c00f2e4)
- contextual embedding layer가 단어의 사용을 좀 더 잘 구별했다.

# 5.2 Discussion
![result4](https://files.slack.com/files-pri/T1J7SCHU7-F9CHV2TQA/r4.png?pub_secret=4fac357147)
- 어떤 Qeury에 어떤 Context vector가 담겼는지를 보여줌

# 5.3 Error Analysis
- 50% : imprecise boundaries of the answers
- 28% : syntactic complication/ambiguitis
- 14% : paraphrase problems
- 4% : require external knowledge
- 2% : need multiple sentences to answer
- 2% : mistake during tokenization

-----

## 6. Cloze Test Experiment
![result5](https://files.slack.com/files-pri/T1J7SCHU7-F9CEY73NX/r5.png?pub_secret=e035b0a44a)

-----

## 7. Conclusion
- ablation analysis에서 확인했듯이, model의 구성요소가 모두 중요하다. 
- **context summarization 없이 query-aware context representaion을 찾아냈다.**

-----

## 8. Reference
- [https://arxiv.org/abs/1611.01603](https://arxiv.org/abs/1611.01603)
- Ybigta deepNLP-study
