---
layout: post
title:  "charCNN[2] Character Aware Neural Language Model(2016) - Review"
date:   2018-02-27 17:30:00 +0900
categories: [deeplearning, cnn, charcnn, nlp, paperreview]
---

## 1. Abstract
- CNN, RNN,LSTM, Highway Network 모델 사용
- 형태소가 많은 언어에 유리(아랍어, 체코어, 프랑스어, 독일어, 스페인어, 러시아어 등)
- character단위의 모델링은 semantic, orthographic information(철자법)을 동시에 알아낼 수 있음

-----

## 2. Introduction
- 이전까지는 Markov n-gram based Model이 많이 사용되었지만, sparsity data문제가 발생 
- Neural Language Model(NLM)에서는 word embedding을 사용하여 sparsity data문제를 해결
    - word embedding을 통해 비슷한 단어는 비슷한 공간에 위치하게 됨
- 하지만, word embedding으로는 subword information를 파악할 수 없음
    - infrequent word에 대해서 perplexity가 높아지게 됨(성능이 좋지 않음)
    - 이러한 점은 형태소가 많은 언어에서 문제가 됨
- **이 논문에서는 charCNN을 통해 subword information를 잘 학습할 수 있도록하는 것이 목적**
    - 전처리 작업에서 morphological-tagging 작업이 필요하지 않음

- **결론적으로, 형태소가 많은 언어에서 이 모델이 더 적은 parameter로도 다른 LSTM 기반 모델보다 성능이 좋았다.**

-----

## 3. Model
# 3.1 Architecture
![architecture](https://files.slack.com/files-pri/T1J7SCHU7-F9FEBJ44D/model.png?pub_secret=cf90fc7e14)
- charCNN, highway Network, LSTM 사용

# 3.2 RNN, LSTM, RNN-LM
### 3.2.1 RNN
- $$h_{t}$$ = $$f(Wx_{t} + Uh_{t-1} + b)$$ 

### 3.2.2 LSTM
- $$i_{t}$$ = $$\sigma (W^{i} x_{t} + U^{i} h_{t-1} + b^{i})$$
- $$f_{t}$$ = $$\sigma (W^{f} x_{t} + U^{f} h_{t-1} + b^{f})$$
- $$o_{t}$$ = $$\sigma (W^{o} x_{t} + U^{o} h_{t-1} + b^{o})$$
- $$g_{t}$$ = $$tanh(W^{g} x_{t} + U^{g} h_{t-1} + b^{g})$$
- $$c_{t}$$ = $$f_{t} \odot c_{t-1} + i_{t} \odot g_{t}$$
- $$h_{t}$$ = $$o_{t} \odot tanh(c_{t})$$

### 3.2.3 RNN-LM
- Language Model에서의 scoring
- negative log-likelihood(NLL)을 최소화하는 방향으로 학습
    - $$NLL$$ = $$- \sum_{t=1}^{T} log (p(w_{t} \mid w_{1:t-1}))$$ 
    - $$p(w_{t} \mid w_{1:t-1})$$ = $$frac{exp(P^{j} h_{t} + q^{j})}{\sum_{j'=1}^{V} exp(P^{j'} h_{t} + q^{j'})}$$
    - $$V$$ = 단어의 수, $$j$$ = j-th column 


# 3.3 charCNN
- 전체적인 model과 함께 학습됨
- input 
    - embedding한 characters를 이어붙인 matrix, $$Q \in R^{d \times C}$$
    - $$d$$ = character의 embedding dimension, $$C$$ = word를 구성하는 character의 개수
- convolution
    - n-gram의 역할, prefix, suffix 등을 구별하는 역할을 하게 됨
    - 100~1000개의 filter로 convolution
    - activation function으로 $$tanh$$ 사용
- pooling 
    - convolution의 결과에 pooling을 사용하여 정보를 압축 
    - pooling의 결과가 word embedding이 됨

# 3.4 Highway Network
- residual network의 general한 형태
- $$z$$ = $$t \odot g(W_{H}y + b_{H} + (1-t) \odot y)$$
    - transform gate $$t$$ = $$\sigma (W_{T}y + b_{T})$$ 
    - transform gate도 전체적인 model과 함께 학습됨 

-----

## 4. Experimental Setup
- PPL(perplexity)
    - $$PPL$$ = $$exp(frac{NLL}{T})$$
    - $$T$$ = word of sequence

- English Penn Treebank(PTB) dataset을 사용하여 parameter를 조정한 후, 이것을 다른 언어에 적용
- singleton word만 <unk>로 바꿈

# 4.1 Optimization
![architecture2](https://files.slack.com/files-pri/T1J7SCHU7-F8Y3JUETC/architecture2.png?pub_secret=d34c04a649)
- stochastic gradient descent(SGD) 사용
- dropout 사용
- gradient norm $$norm$$ > $$5$$일 경우, $$norm$$ = $$5$$

-----

## 5. Result

# 5.1 English Penn Treebank
![result](https://files.slack.com/files-pri/T1J7SCHU7-F8Y3PK2QJ/result_table.png?pub_secret=608c35e027)

# 5.2 Other Language
- ![result1](https://files.slack.com/files-pri/T1J7SCHU7-F8XCFANGG/result1.png?pub_secret=03e5bde7c4)
- ![result2](https://files.slack.com/files-pri/T1J7SCHU7-F8Y5TRL1K/result2.png?pub_secret=45ebdd66a6)

-----

## 6. Discussion
# 6.1 Learned Word Representation
![result3](https://files.slack.com/files-pri/T1J7SCHU7-F8XFVHEJV/result3.png?pub_secret=03eda553eb)
- highway layer가 있는 것이 word embedding 학습을 더 잘했음
- OOV word도 학습을 잘함

# 6.2 Learned Character N-gram Representation
![result4](https://files.slack.com/files-pri/T1J7SCHU7-F8Y2B0VCL/result4.png?pub_secret=f5d4c3139f)
- charCNN을 사용한 경우 각 단어의 n-gram에서 prefix, suffix, hypenated를 각각 구별
    - prefix = 접두사, suffix = 접미사, hyphenated = 하이픈으로 연결된 단어

# 6.3 Highway Layer
![result5](https://files.slack.com/files-pri/T1J7SCHU7-F8Y2E2E3E/result5.png?pub_secret=dd3dfdf1ac)
- MLP의 성능은 좋지 않음
- 1개의 highway layer만 가진 것이 제일 성능이 좋다.
- max-pooling 이전에 convolution layer를 더 쌓아도 성능이 좋아지지 않는다.
- CNN 없이 highway network만 사용하면 소용없다.

# 6.4 Effect of Corpus/Vocab size
- corpus의 크기가 커지면 perplexity가 줄어든다. 즉, 성능이 좋아진다.

# 6.5 Further Observation
- 위 실험에서는 word embedding과 charCNN을 같이 사용한 모델이 더 성능이 안좋았다.
- 다른 모델보다 시간은 오래걸리지만, charCNN-layer를 공유하기 때문에 GPU사용 효율성이 증가한다.

-----

## 7. Related Work
- NLM
- FNLM
- char NLM
- DCNN
- CRF
- BDLSTM

-----

## 8. Conclusion
단어의 subword information을 파악하는 것이 중요하다.

-----

## 9. Reference
- [https://arxiv.org/abs/1508.06615](https://arxiv.org/abs/1508.06615)
- Ybigta deepNLP-study
