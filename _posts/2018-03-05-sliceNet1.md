---
layout: post
title:  "sliceNet[1] Depthwise Separable Convolutions for Neural Machine Translation(2017) - Review"
date:   2018-03-05 23:59:00 +0900
categories: [deeplearning, slicenet, paperreview]
---

## 1. Abstract
- Depthwise separable convolution 사용
    - [Xception](https://hwkim94.github.io/deeplearning/cnn/inception/xception/paperreview/2018/03/25/Xception1.html)으로 부터 영향을 받음
    - parameter와 연산의 수를 줄이지만, 설명력은 높여줌
- convolution의 window를 증가시키고, [byteNet](https://hwkim94.github.io/deeplearning/bytenet/paperreview/2018/03/05/byteNet1.html)의 dilation을 모델에서 삭제

-----

## 2. Introduction
- auto-regressive convolution model
    - RNN과 같이 순차적으로 처리하지 않아도, 넓은 범위의 time을 고려할 수가 있음
    - 하지만, convolution을 위한 parameter증가와 연산량의 증가가 문제가 됨
    - 이를 해결하기 위하여 Depthwise separable convolution 사용

-----

## 3. Contribution
- Depthwise separable convolution with residual connection
    - parameter를 효과적으로 줄임
    - channel과 space를 따로 연산
- Super-separable convolution
    - Sub-separable convolution(grouped convolution)의 변형
- dilation기법 삭제
    - convolution의 window를 크게 만들어서 삭제

# 3.1 Separable convolutions and grouped convolutions
- regular convolution
    - $$Conv(W,y)_{(i,j)}$$ = $$\sigma_{w,h,d = 0}^{W,H,D} W_{(w,h,d)} \cdot y_{(i+w,j+h,d)}$$
    - **filter의 크기 = $$W \times H \times D$$**

- pointwise convolution
    - $$PointwiseConv(W,y)_{(i,j)}$$ = $$\sigma_{d = 0}^{D} W_{d} \cdot y_{(i,j,d)}$$
    - 1x1 convolution
    - $$y_{(i,j,1)} ~ y_{(i,j,D)}$$의 정보를 합침, 즉, 모든 channel의 정보만 연산
    - **filter의 크기 = $$1 \times 1 \times D$$**

- depthwise convolution
    - $$DepthwiseConv(W,y)_{(i,j,k)}$$ = $$\sigma_{w,h = 0}^{W,H} W_{(w,h)} \cdot y_{(i+w,j+h,k)}$$
    - channel과는 독립적인 spatial한 convolution
    - d-th channel의 $$y_{(i,j,d)}$$ ~ $$y_{(i+w,j+h,d)}$$의 정보를 합침, 즉, 한 channel내의 spatial한 정보만 연산 
    - **filter의 크기 = $$W \times H \times 1$$**

- depthwise separable convolution 
    - $$DepthwiseSepConv(W_{p}, W_{d}, y)_{(i,j)}$$ = $$PointwiseConv(W_{p}, 
DepthwiseConv(W_{d}, y)_{(i,j)})_{(i,j)}$$
    - depthwise convolution을 한 후, pointwise convolution
    - spatial feature combination step과 channel combination step의 분리
    - 같은 channel내의 feature끼리는 highly correlated 되어 있지만, 각 channel끼리는 다른 filter를 적용했으므로 독립적이다. 따라서, 각 channel의 feature를 연산한 후 각 channel간의 정보를 나중에 합쳐야한다. 하지만 기존의 convolution은 이 두 개를 한 번에 처리해야 했으므로 inefficient하며 feature의 특성을 제대로 반영할 수 없다. 
    - **filter의 크기 = $$W \times H  + D$$**

- sub-separable convolution(grouped convolution)
    - $$SupSepConv(W,y)_{(i,j,k)}$$ = $$\sigma_{w,h,d = 0}^{W,H,\frac{D}{g}} W_{(w,h,d)} \cdot y_{(i+w,j+h,kg+d)}$$
    - input tensor의 channel을 g등분한 후, 각각에 대하여 regular convolution을 하고, 다시 concatenation
    - regular convolution과 depthwise separable convolution의 중간에 위치
    - **filter의 크기 = $$\frac{W \times H \times D}{g}$$**

# 3.2 Super-separable convolutions
- super-separable convolution
    - $$SuperSepConv(W_{p}, W_{d}, y)_{(i,j,k)}$$ = $$DepthwiseSepConv(W_{p}^{k}, W_{d}^{k}, y[i,j,kg:(k+1)g])_{(i,j)}$$
    - input tensor의 channel을 g등분한 후, 각각에 대하여 depthwise separable convolution을 하고, 다시 concatenation
    - **filter의 크기 = $$W \times H + \frac{D}{g}$$**

# 3.3 Parameter count comparison across convolution types
<table>
  <tr>
    <th align="center">Convolution</th>
    <th align="center">Filter Size</th>
    <th align="center">Number of Parameters</th>
  </tr>

  <tr>
    <td>regular</td>
    <td align="center">$$W \times H  \times D$$</td>
    <td align="center">$$(n^{2} \times c) \times c$$</td>   
  </tr>

  <tr>
    <td>depthwise separable</td>
    <td align="center">$$W \times H  + D$$</td>
    <td align="center">$$(n^{2} + c) \times c$$</td>   
  </tr>

  <tr>
    <td>g-sub-separable</td>
    <td align="center">$$\frac{W \times H \times D}{g}$$</td>
    <td align="center">$$(n^{2} \times \frac{c}{g}) \times c$$</td>   
  </tr>

  <tr>
    <td>g-super-separable</td>
    <td align="center">$$W \times H + \frac{D}{g}$$</td>
    <td align="center">$$(n^{2} + \frac{c}{g}) \times c$$</td>   
  </tr>
  <tr>
      <th colspan="3">$$n$$ = filter width&height, $$c$$ = input&output channel, $$g$$ = number of group</th>
  </tr>
</table>

# 3.4 Filter dilation and convolution window size
- dilation 
    - convolution의 receptive field를 parameter의 증가 없이 넓혀주기 위하여 사용
    - 주로 auto-regressive convolution model에서 사용

- dilation의 단점
    - dilation을 하게되면, checkboard artifacts와 같이 특정 부분만 여러번 연산되는 문제가 발생한다.
    - 따라서, 적게 포함 되는 부분은 상대적으로 중요하지 않게 여겨져 dead zone이 되어버린다.

- 해결책 by increasing window size
    - 애초에 dilation은 receptive field를 넓히기 위해 고안 된 것
    - 따라서, convolution의 window를 넓혀주면 unequal convolutional coverage 문제가 사라짐
    - **그동안은 parameter 수의 증가가 문제가 되었지만, depthwise separable convolution을 통해 parameter가 많이 감소했으므로, window 크기를 증가시켜도 parameter의 수가 크게 문제되지 않음**

-----

## 4. SliceNet Architecture
![slicenet](https://files.slack.com/files-pri/T1J7SCHU7-F9ZDGF0D6/slicenet.png?pub_secret=d62cb4e460)
- input과 output을 다른 NN에서 embedding 한 후, decoding 하기 전에 concatenate하는 방식으로 auto-regressive한 모델을 만듦

# 4.1 Convolutional modules
- Convolution step
    - $$ConvStep(W, x)$$ = $$LN(SeparableConv(W, ReLU(x)))$$
    - ReLU -> Depthwise separable convolution -> Layer Norm의 순서로 구성
- Convolution module
    - $$hidden1(x)$$ = $$ConvStep(W_{1}, x)$$
    - $$hidden2(x)$$ = $$x + ConvStep(W_{2}, x)$$
    - $$hidden3(x)$$ = $$ConvStep(W_{3}, hidden2(x))$$
    - $$hidden4(x)$$ =  $$x + ConvStep(W_{4}, hidden3(x))$$
    - $$ConvModule(x)$$ = $$Dropout(hidden4(x), dropout_rate)$$
    - $$ConvModule^{k}(x)$$ = k-stacked $$ConvModule(x)$$
    - Convolution step과 resNet으로 구성

# 4.2 Attention modules
- $$Attend(source, target)$$ = $$\frac{1}{\sqrt{hidden_size}} \cdot Sofrmax(target \cdot {source}^{T}) \cdot source$$
    - $$source$$ shape = [input_length, hidden_size]
    - $$target$$ shape = [output_length, hidden_size]
    - inner product attention

- timing
    - $$timing(t, 2d)$$ = $$sin(t/{10000}^{2d/depth})$$
    - $$timing(t, 2d+1)$$ = $$cos(t/{10000}^{2d/depth})$$
    - 위의 sin function, cos function을 concatenate하여 timing vector를 만듦
    - $$timing$$ shape = [k, hidden_size]
    - positional information을 부여(cos과 sin이 있으면 정확한 위치를 알 수 있으므로)

- Attention modules
    - $$attention1(x)$$ = $$ConvStep(W, x + timing)$$
    - $$attention(source, target)$$ = $$Attend(source, ConvStep(W, attention1(target)))$$
    - target에 timing이 더해지므로, source의 각 vector가 어느 곳으로 주로 가는지를 파악할 수 있음

# 4.3 Auto-regressive structure
- Long term dependency
    - Long term dependency를 가지는 경우 NMT에서 좋은 성능을 보여줌
    - auto-regressive model은 바로 전에 생성된 output뿐만 아니라 이전의 모든 output에 의존하므로 long term dependency가 높은 편
    - **sliceNet은 window를 넓혀 recpetive field의 크기를 증가시켜 long term dependency를 높임**

- InputEncoder
    - $$InputEncoder(x)$$ = $${ConvModule}^{6}(x+timing)$$
- IOMixer
    - $$IOMixer(i,o)$$ = $$ConvStep(W, [attention(i,o); o])$$
- AttnConvModule
    - $$AttnConvModule(x, source)$$ = $$ConvModule(x) + attention(source, x)$$
- Decoder
    - $$Decoder(x)$$ = $${AttnConvModule}^{4}(x, InputEncoder(source))$$

- Model Architecture
    - $$mix$$ = $$$IOMixer(InputEncoder(input), OutputEmbedding(output))$$
    - $$output$$ = $$Decoder(mix)$$

-----

## 5. Related Work
- Xception
- byteNet
- waveNet
- mobileNet

-----

## 6. Experiments
![result](https://files.slack.com/files-pri/T1J7SCHU7-FA12RPHGW/result1.png?pub_secret=929b53a31a)
- depthwise separable convolution의 성능이 paramter의 수가 regular convolution보다 적지만, 성능은 훨씬 좋다.
- sub-separable convolution은 depthwise separable convolution보다 살짝 성능이 안좋다.
    - group의 크기가 작아질수록 성능이 좋아지는데, 이것은 depthwise separable convolution에 가까워지기 때문이다.
- dilation의 필요성이 사라졌다.
    - dilation을 없애고, window의 크기를 키우면 성능이 더 좋아지거나, 최소한 유지된다.
- super-separable convolution의 성능이 depthwise separable convolution보다 좋다.

-----

## 7. Conclusion
- dilation은 굳이 필요하지 않은 기법이다. 
    - depthwise separable convolution을 통해서 더 넓은 window를 가진 convolution을 사용하면 된다.
- 모든 regular convolution이 depthwise separable convolution으로 대체되길 기대한다.

-----

## 8. Reference
- [https://arxiv.org/abs/1706.03059](https://arxiv.org/abs/1706.03059)
- [https://blog.yani.io/filter-group-tutorial/](https://blog.yani.io/filter-group-tutorial/)
- [https://distill.pub/2016/deconv-checkerboard/?utm_source=mybridge&utm_medium=blog&utm_campaign=read_more](https://distill.pub/2016/deconv-checkerboard/?utm_source=mybridge&utm_medium=blog&utm_campaign=read_more)
- Ybigta deepNLP-study
