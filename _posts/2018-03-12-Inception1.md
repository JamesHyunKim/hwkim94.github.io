---
layout: post
title:  "Inception[1] Going Deeper with Convolutions(2014) - Review"
date:   2018-03-12 03:59:00 +0900
categories: [deeplearning, cnn, inception, paperreview]
---

## 1. Abstract
- Inception이란 GoogLeNet에 탑재된 모듈, GoogLeNet은 2014년 IRSVRC에서 1등을 한 모델
- **모델의 depth와 width는 늘리며, CNN에서 parameter는 줄여 연산량은 줄이는 것이 목적**

-----

## 2. Introduction
Inception 모듈의 목적은 더 깊은 network를 효율적으로 만드는 것

-----

## 3. Related Work
- NIN(Network in Network)

-----

## 4. Motivation and High Level Considerations
- Deep NN의 성능을 올리기 위해서는 depth(layer의 수)와 width(각 layer에 있는 unit의 수)를 증가시켜야함
- depth와 width를 무작정 증가시킬 경우 overfitting과 연산량이 너무 많아진다는 문제점이 발생
    - 따라서, Fully-connected architecture 보다 Sparsely-connected architecture를 사용해야 함
    - ex) NN이 chained 될 경우, 각 layer의 parameter의 수가 곱해지기 때문에 연산량이 크게 증가
- uniform분포를 가진 데이터가 sparse deep NN으로 잘 표현된다면, 많이 활성화된 unit들을 분석하여 최적 효율의 topology를 구성 가능
    - 하지만, 실제 데이터들은 uniform분포를 따르지 않기 때문에 dense Matrix가 필요
- **따라서, Sparse Matrix를 Dense Submatrix의 Cluster로 구성하여 layer간의 연결은 sparse하지만 layer 내부적으로는 dense하게 구성**
    - 작지만 dense한 연산인 convolution 여러 개로 쪼개어 표현하게 된다면 한 layer내에서의 연산은 dense함 
    - FC처럼 모든 unit마다 weight가 주어지는 것이 아니기 때문에 layer끼리는 상대적으로 sparse함
    - 즉, layer끼리는 sparse하게 연결되지만, 사용되는 행렬자체는 dense
 
-----

## 5. Architectural Details
![architecure](https://files.slack.com/files-pri/T1J7SCHU7-F9MUD8WE7/inception.png?pub_secret=a940b11d99)
- CNN
    - 모든 unit에 weight를 곱해주는 것이 아니라서 sparse하지만, 국지적으로는 dense한 구조를 사용
- size of filter
    - filter의 크기가 너무커지면, 여러 정보가 한 곳으로 다 모이기 때문에 특징을 잘 잡아낼 수 없음
    - 이미지의 크기가 빠르게 줄어들게 됨
    - 따라서 1x1, 3x3, 5x5 convolution만 사용
- 1x1 convolution
    - 연산량을 효과적으로 줄이기 위하여 사용
    - 1x1 convolution을 하게 되면 같은 filter의 결과로 나타내져 있는 height, width에 펼쳐진 정보(주변 정보)는 고려하지 않고, 한 지점에 대한 여러 filter들의 정보만(feature-map)을 고려하게 됨
    - 따라서 차원을 줄이면서 그 정보들을 통합하는 역할을 함
    - ex) 이미지의 경우 한 픽셀에 RGB형태로 세가지 feature-map이 있는데, 1x1 convolution을 통해 한 개 혹은 두 개의 feature-map으로 줄여줌    

- Activation
    - ReLU

-----

## 6. GoogLeNet
![googlenet](https://files.slack.com/files-pri/T1J7SCHU7-F9MF2U1DF/model.png?pub_secret=8313e803d3)

# 6.1 Parameter
![param](https://files.slack.com/files-pri/T1J7SCHU7-F9MF0FWJV/parameter.png?pub_secret=aa21e6707a)
- 모든 3x3, 5x5 convolution 이전에 1x1 convolution을 해줘서 dimension을 줄임
- 5x5 convolution이 3x3 convolution보다 훨씩 적음
    - 5x5 convolution의 연산량이 많기 때문에 이를 줄여주기 위해서 적게 사용
    - 이미지의 크기가 크지 않으므로, 3x3 convolution이 더 세밀한 정보를 많이 파악할 수 있어서 유리

# 6.2 Average pooling
- parameter의 수를 효과적으로 줄이기 위해 사용
    - **마지막 Fully-connected는 많은 비중의 parameter를 차지하기 때문에 연산량도 많고, overfitting의 위험성도 존재**
- 이미 vector들이 본래의 data에 대한 충분한 정보(feature)를 가지고 있기 때문에, average pooling을 사용해도 정보의 손실 없이 vector를 구성할 수 있음
- 최종단에서 Fully-connected 이전에 사용

# 6.3 Auxiliary classifier
![aux](https://files.slack.com/files-pri/T1J7SCHU7-F9N1278H2/auxilary.png?pub_secret=67548afa3a)
- vanishing gradient를 해결하기 위하여 존재
- 망이 깊어질수록, input layer쪽에는 gradient가 전달되지 않으므로, 중간중간에 classifier를 설치하여 gradient를 갱신
    - **학습과정에서 propagation될때, 최종적인 gradient가 해당 layer를 지나면서 이곳의 gradient가 더해짐**
    - 하지만, 제일 중요한 것은 최종의 결과이므로, 0.3을 곱해서 사용
- regularization효과도 있음
- 학습이 끝나면 제거

-----

## 7. Training Methodology
대회를 진행하는 중간중간에 hyperparameter를 계속 수정했기 때문에, 마지막에 사용된 hyperparameter가 최종결과를 만들었다고 할 수 없으므로 값을 어떻게 설정했는지 구체적으로 밝히지 않음

-----

## 8. ILSVRC 2014 Classification Challenge Setup and Results
![result1](https://files.slack.com/files-pri/T1J7SCHU7-F9MC1E5BJ/result.png?pub_secret=7f335d4a68)

-----

## 9. ILSVRC 2014 Detection Challenge Setup and Results
![result2](https://files.slack.com/files-pri/T1J7SCHU7-F9N1WBG02/result22.png?pub_secret=7b3bc1518a)

-----

## 10. Conclusions
*"moving to sparser architectures is feasible and useful"*

-----

## 11. Reference
- [https://arxiv.org/abs/1409.4842](https://arxiv.org/abs/1409.4842)
- [https://norman3.github.io/papers/docs/google_inception](https://norman3.github.io/papers/docs/google_inception)
- [https://laonple.blog.me/220686328027](https://laonple.blog.me/220686328027)


