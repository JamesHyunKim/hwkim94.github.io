---
layout: post
title:  "Inception[2] Rethinking the Inception Architecture for Computer Vision(2015) - Review"
date:   2018-03-13 03:59:00 +0900
categories: [deeplearning, cnn, inception, paperreview]
---

## 1. Abstract
- **convolution을 factorizing(분해)하여 더 효율적인 Network를 구성**
- Inception 모듈을 개선하여 Inception-v2, Inception-v3 제시

-----

## 2. Introduction
- 2014년 ILSVRC에서 좋은 성적을 보여준 VGGNet과 GoogLeNet은 2012년 ILSVRC의 우승자인 AlexNet보다 더 깊어졌고, 성능도 개선됨
- GoogLeNet은 VGGNet이나 AlexNet보다 parameter의 수가 적으며, 연산량도 훨씬 줄어들게 되므로 더 효율적
- 하지만, GoogLeNet은 구조가 복잡하여 응용하거나 변형하기에 어려움
- 이 논문에서는 Inception을 변형하여 효율성을 증대시킴

-----

## 3. General Design Principles
1. Avoid representational bottlenecks, especially early in the network.
    - 병목구간처럼 데이터가 축약됐다가 다시 들어나는 형태는 바람직하지 않음
    - 즉, 데이터는 전체적이 Network에 따라서 서서히 차원이 줄어들어야 함

2. Higher dimensional representations are easier to process locally within a network.
    - 차원의 크기가 크다면, 국지적으로 처리하는 것이 편하다.
    - 따라서, convolution을 할 때 더 국지적으로 집중을 더 잘 할 수 있고 feature를 더 많이 추출할 수 있다.
    - 뿐만 아니라, 학습도 빠르게 해주는 효과가 있다.

3. Spatial aggregation can be done over lower dimensional embeddings without much or any loss in representational power.
    - 한 embedding을 낮은 차원의 embedding으로도 공간적 통합을 하는 것은 정보의 손실이 거의 없다.
    - 차원을 축소하더라도, 인접한 unit과의 상관관계는 소실되지 않음. 즉, 정보전달에 큰 영향이 없음
    - 차원의 감소는 학습을 빠르게 한다.

4. Balance the width and depth of the network.
    - layer의 depth와 width 모두 Network에서 중요하다. 하지만, 컴퓨터 자원은 제한되어 있으니 잘 분배되어야한다.

-----

## 4. Factorizing Convolutions with Large Filter Size
- GoogLeNet의 좋은 결과는 dimension reduction의 영향이 크다.
    - 3x3, 5x5 convolution 이전에 1x1 convolution을 통한 dimension reduction
- parameter의 수를 줄여 computational cost를 줄임
    - 더 빠른 학습이 가능
    - 절약된 컴퓨터 자원은 모델의 성능을 더 높이는 것에 사용 될 수 있음

# 4.1 Factorization into smaller convolutions
![5x5](https://files.slack.com/files-pri/T1J7SCHU7-F9NEWEP50/model.png?pub_secret=6f41126c70)
- 5x5, 7x7 convolution은 computation의 관점에서 expensive하지만, layer의 feature를 쉽게 찾아낼 수 있다는 장점이 있음
    - filter의 size를 무작정 줄이게 된다면 연산량은 줄어들지만, 정보의 expressiveness(표현성)을 잃게 됨
- **크기가 큰 convolution은 작은 형태의 convolution 여러 개로 나눌 수 있음(factorization)**
    - 정보의 표현성은 같은데, 연산량이 줄어들게 되므로 효율적
    - ex) 5x5 convolution은 2개의 연속된 3x3 convolution으로 나타낼 수 있음 
    - ex) 5x5 convolution의 연산량 = 25
    - ex) consecutive 3x3 convolution의 연산량 = 9+9 = 18

# 4.2 Spatial Factorization into Asymmetric Convolutions
![model2](https://files.slack.com/files-pri/T1J7SCHU7-F9MUCS7FB/model2.png?pub_secret=a3380adfaf)
- **nxn convolution은 1xn, nx1 convolution으로 표현가능**
    - n=7일 때가 가장 성능이 좋았음
    - ex) 3x3 convolution은 1x3, 3x1 convolution으로 표현

# 4.3 Result of Factorization
### 4.3.1 Result of Factorization
![i1](https://files.slack.com/files-pri/T1J7SCHU7-F9NF53MNG/inin.png?pub_secret=5b142f90f7)

### 4.3.2 Result of Asymmetric Convolutions
![i2](https://files.slack.com/files-pri/T1J7SCHU7-F9NFZ0S5Q/ininin.png?pub_secret=6a013e05f2)

-----

## 5. Utility of Auxiliary Classifiers
![ac](https://files.slack.com/files-pri/T1J7SCHU7-F9MS44DMW/clas.png?pub_secret=926678c8c7)
- 실험 결과, 제일 하단의 Auxiliary Classifier는 학습에 영향을 주지 않는다는 것을 발견하였기 때문에 모델에서 제거
- Auxiliary Classifiers는 regularization 역할을 수행

-----

## 6. Efficient Grid Size Reduction
![grid](https://files.slack.com/files-pri/T1J7SCHU7-F9NKM99BP/grid.png?pub_secret=5b3fe42e31)
- 일반적으로 CNN에서는 Grid size를 줄이기 위하여 pooling을 사용
- Pooling을 Inception보다 먼저 할 경우
    - pooling을 통해 width와 height가 줄었다가, Inception을 통해 depth가 늘어나므로 bottleneck 형성
- Inception을 Pooling보다 먼저 할 경우
    - grid를 줄이지 않은 채 연산을 하므로, 연산량이 많아지게 됨
- 따라서, Inception과 Pooling을 병렬적으로 연산한 후 concatenate
    - 차원의 수를 맞춰줘야 하므로 Inception에서 stride=2 convolution 사용

# 6.1 Result of Grid Size Reduction
![rgird](https://files.slack.com/files-pri/T1J7SCHU7-F9MS7BQM6/rgrid.png?pub_secret=04ede128f6)

-----

## 7. Inception-v2
![architecture](https://files.slack.com/files-pri/T1J7SCHU7-F9NGCMVH8/inceptionv2-ar.png?pub_secret=f760132107)
![param](https://files.slack.com/files-pri/T1J7SCHU7-F9MVBJZA5/inceptionv2.png?pub_secret=4f2b9b1a80)

# 7.1 Paramters
![param](https://files.slack.com/files-pri/T1J7SCHU7-F9MVBJZA5/inceptionv2.png?pub_secret=4f2b9b1a80)
- 7x7 convolution을 3x3 convolution으로 factorization

# 7.2 Inception module
![archi](https://files.slack.com/files-pri/T1J7SCHU7-F9PJ15SVD/modules.png?pub_secret=cad160e68e)
- 3종류의 Inception module 사용

-----

## 8. Model Regularization via Label Smoothing
- target을 one-hot encoding을 하지않고, 1이 아닌 곳에 noise $$\epsilon$$를 추가해줘서 regularization
    - target = $$(\epsilon, \epsilon, \cdots , 1 - (n-1)\epsilon, \cdots , \epsilon)$$
    - loss $$H(q',p)$$ = $$( 1-\epsilon )H(q,p) + \epsilon H(u,p)$$ 
    - $$q'(k)$$=$$( 1-\epsilon )q(k) + \epsilon u(k)$$
    - $$u(k)$$ = 고정된 k의 확률분포 = noise 역할 = uniform 1/K
    - $$p$$ = logit, $$k$$ = target vector의 k-th element
    - one-hot encoding의 경우 0 혹은 1 밖에 없으므로, 정답이 틀릴 경우 penalty가 아주 크게 됨
    - 즉, q는 x가 input으로 주어졌을 때 결과의 확률분포를 의미하는데, 이 값을 조금 줄여주고 모든 확률분포에 noise를 더해줘서 정답의 확률은 낮춰주고, 오답의 확률은 높혀줘 정답을 틀려도 penalty를 조금만 주도록 설계
    - 확률분포의 그래프를 생각해보면, 전체적으로 $$1-\epsilon$$의 비율만큼 줄어들게 되는데, 높은 확률을 가질 수록 많이 줄어들게 된다. 즉, 확률분포의 그래프가 조금 평평해지게 된다.
    - loss값이 극단적으로 커지지 않도록 해주는 역할 

-----

## 9. Inception-v3
- 구조는 Inception-v2와 같음
- 여러가지의 Inception-v2 실험 중, 성능이 좋은 것을 Inception-v3라고 지칭
    - RMSProp
    - Label Smoothing
    - 7x7 convolution Factorization
    - BN-auxiliary

-----

## 10. Training Methodology
- 100 epoch
- batch size 32
- RMSProp
- learning rate 0.045

-----

## 11. Performance on Lower Resolution Input
![rsln](https://files.slack.com/files-pri/T1J7SCHU7-F9NG8TDSQ/a.png?pub_secret=4089a13014)
- receptive field, 즉, resolution에 상관없이 Network의 computation cost가 비슷하다면 비슷한 결과를 가져옴
- 따라서, resolution에 따라서 Network의 size를 줄이게 되면 성능이 안좋아짐

-----

## 12. Experimental Results and Comparisons
![result](https://files.slack.com/files-pri/T1J7SCHU7-F9NLAPSMT/result.png?pub_secret=6fac087c46)

-----

## 13. Conclusions
- *"Factorizing convolutions and aggressive dimension reductions inside neural network can result in networks with relatively low computational cost while maintaining high quality"*
- *"The combinationof lower parameter count and additional regularization with batch-normalized auxiliary classifiers and label-smoothing allows for training high quality networks on relatively modest sized training sets"*

-----

## 14. Reference
- [https://arxiv.org/abs/1512.00567](https://arxiv.org/abs/1512.00567)
- [https://norman3.github.io/papers/docs/google_inception](https://norman3.github.io/papers/docs/google_inception)
- [https://laonple.blog.me/220686328027](https://laonple.blog.me/220686328027)
