---
layout: post
title:  "Inception[3] Inception v4, Inception ResNet and the Impact of Residual Connections on Learning(2016) - Review"
date:   2018-03-17 02:00:00 +0900
categories: [deeplearning, cnn, inception, paperreview]
---

## 1. Abstract
- Inception은 비교적 적은 연산비용으로 좋은 성능을 발휘한다.
- resNet을 사용하여 Inception의 학습속도를 증가시킬 수 있다.

-----

## 2. Introduction
- resNet과 Inception을 통합하여 Inception-resNet 모델 제시
- Inception모듈도 발전시켜 Inception-v4모델 제시

-----

## 3. Related Work
![res](https://files.slack.com/files-pri/T1J7SCHU7-F9R5M8JR0/resnet.png?pub_secret=bff03f6bfa)
- [resNet](https://hwkim94.github.io/deeplearning/cnn/resnet/paperreview/2018/02/10/resNet1.html)
- [Inception-v3](https://hwkim94.github.io/deeplearning/cnn/inception/paperreview/2018/03/13/Inception2.html)

-----

## 4. Architectural Choices
# 4.1 Pure Inception blocks
- 여태까지는 메모리 문제때문에 subnetwork로 분리되어 각각 학습되었음
    - 각 subnetwork의 연산량과 속도를 맞추기 각 layer에 위하여 filter수를 변경해줬어야 했음
- Tensorflow 덕분에 모델을 따로 나누지 않고 학습 가능
    - 따라서 같은 grid크기를 가지는 module로 이루어진 획일적인 model 구성이 가능해짐

# 4.2 Residual Inception Blocks
- resNet을 적용하며 기존의 Inception module을 간단하게 변형
    - **모든 Inception block은 1x1 convolution으로 시작하여 연산량을 감소시킴**
    - **마지막에 1x1 conolution을 통하여 depth를 늘려줌(Reduction을 통해 차원이 줄어들었기 때문)**
- 마지막 layer를 제외하고는 Batch Normalization을 사용하지 않음
    - BN은 많은 자원을 소모하므로 제거하고, 대신에 더 많은 Inception Block을 추가해줌

# 4.3 Scaling of the Residuals
![scale](https://files.slack.com/files-pri/T1J7SCHU7-F9SB2TW87/scale.png?pub_secret=6184d6aba1)
- filter의 수가 1000개를 넘어가면 residual(잔차)들이 불안정해져 avg pooling 결과가 0으로 나오는 현상 발생
    - learning rate를 낮추거나 BN을 해주는 것만으로는 해결 불가
    - residual을 0.1~0.3 수준으로 rescale을 해주면 안정적으로 학습이 가능

-----

## 5. Inception-v4
![v4](https://files.slack.com/files-pri/T1J7SCHU7-F9S7324MV/v4.png?pub_secret=c8f11f404a)
- **Inception모듈에서는 차원의 변화가 없으며 Reduction모듈에서 차원이 줄어들게 됨**
    - Reduction module은 pooling과 convolution은 병렬 연산의 concatenation
- **stem부분의 architecture가 변화**

# 5.1 Stem
![stem](https://files.slack.com/files-pri/T1J7SCHU7-F9S12M19C/stem.png?pub_secret=da59b82017)
- 7x7 convoltion을 3개의 3x3 convolution으로 분해
- 차원을 줄이기 위하여 pooling과 2-stride convolution을 병렬처리
- 1x1 convolution을 통하여 차원을 줄여준 후 convolution
- nxn convolution을 1xn, nx1 convolution으로 분해

# 5.2 Inception-v4 module
![module](https://files.slack.com/files-pri/T1J7SCHU7-F9S7G4UAK/v4-module.png?pub_secret=66ac11032e)
- Inception module에서는 차원이 감소하지 않음, 즉 input과 output의 차원이 같음
- Inception-v3와 거의 유사
    - 8x8 grid module에서만 3x3 모듈이 분해가 됨

# 5.3 Reduction
![reduction](https://files.slack.com/files-pri/T1J7SCHU7-F9S7G3Q87/reduction.png?pub_secret=42241d29bf)
- pooling과 convolution의 병렬 연산으로 차원을 감소

-----

## 6. Inception-resNet
- Inception-v3 + resNet = Inception-resNet-v1
- Inception-v4 + resNet = Inception-resNet-v2
- Inception module에 resNet을 적용
- Reduction 적용

# 6.1 Inception-resNet-v1
![rv1](https://files.slack.com/files-pri/T1J7SCHU7-F9R7Q12F6/rv1.png?pub_secret=5f7e3f2779)

### 6.1.1 Stem
![rv1stem](https://files.slack.com/files-pri/T1J7SCHU7-F9R5PBPPV/rv1stem.png?pub_secret=c03866034f)
- 기존(Inception-v1 ~ Inception-v3)의 Stem을 아주 조금 변형
    - 7x7 convolution의 분해

### 6.1.2 Inception-resNet-v1 module
![rv1m](https://files.slack.com/files-pri/T1J7SCHU7-F9S422C4W/rv1a.png?pub_secret=76de3238d6)
- Inception module이 조금 더 간단한 구조로 변형됨
- resNet처럼 shortcut path가 추가됨

### 6.1.3 Reduction
![rv1r](https://files.slack.com/files-pri/T1J7SCHU7-F9S421622/rv1ra.png?pub_secret=621c154a06)
- Inception-v3에는 없던 Reduction을 추가
- Reduction-A는 Inception-v4와 동일하게 사용됨


# 6.2 Inception-resNet-v2
- 전체적인 Architecture는 Inception-resNet-v1과 동일하며, 그 구성요소가 변형됨
    - Stem의 변형
    - Inception module의 filter수 변경

### 6.2.1 Stem
- Inception-v4의 Stem을 사용

### 6.2.2 Inception-resNet-v2 module
- Inception-resNet-v1 module과 동일한 Architecture을 가졌고, filter의 수만 변경

### 6.2.3 Reduction
- Inception-resNet-v1과 동일한 Reduction사용

-----

## 7. Training Methodology
- RMSProp
- learning rate = 0.045

-----

## 8. Experimental Results
![result](https://files.slack.com/files-pri/T1J7SCHU7-F9R99C22E/result1.png?pub_secret=88134df814)
- 학습속도도 빠르며, 성능도 개선되었다.

-----

## 9. Conclusions
**Inception에 resNet을 적용하여 학습속도를 단축시킬 수 있으며, 성능도 개선 되었다**

-----

## 10. Reference
- [https://arxiv.org/abs/1602.07261](https://arxiv.org/abs/1602.07261)
- [https://norman3.github.io/papers/docs/google_inception](https://norman3.github.io/papers/docs/google_inception)
