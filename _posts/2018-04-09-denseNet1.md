---
layout: post
title:  "denseNet[1] Densely Connected Convolutional Networks(2016) - Review"
date:   2018-04-04 13:59:00 +0900
categories: [deeplearning, cnn, resnet, densenet, paperreview]
---

## 1. Abstract
- 기존에 resNet이 성능이 좋았던 것은 input과 가까운 layer와 output과 가까운 layer가 connected 되어 있기 때문

- denseNet에서는 모든 layer끼리 연결시켜서 model을 구성
    - 즉, preceding layer의 모든 feature map이 subsequent layer의 input이 됨
    - $$L$$개의 layer가 있으면 $$\frac{L(L+1)}{2}$$ 개의 connection 존재

- denseNet의 장점
    - vanishing gradient 해결
    - feature propagation 강화
    - feature의 재사용
    - parameter 수의 감소
    
-----

## 2. Introduction
- CNN 모델은 처음에 제시되었던 것보다 훨씬 깊어지고 있다.
    - 이로 인해 vanishing gradient 문제 발생
    - 이 문제를 해결하는 방안이 많이 제시, 대표적으로 resNet, highway Network, Fractal Net
    - 이러한 모델들의 공통점은 shortcut path를 사용

# 2.1 denseNet
- layer간의 정보 흐름을 최대화하기 위하여 모든 layer를 연결
    - feed-forward한 특성을 보존하기 위하여 previous layer는 모든 subsequent layer의 input으로 사용됨
    - **resNet과 달리 feature map들의 summation을 사용하지 않고, concatenation 사용**
    - 따라서, $$l$$-th layer는 $$l$$개의 input을 가지게 되며,  $$l$$-th layer의 output은 $$L-l$$개의 layer의 input으로 사용됨

- 기존의 모델들보다 더 적은 parameter 사용
    - 기존의 모델들은 많은 parameter를 사용하여 보존되어야 하는 정보와 새로 만들어내는 정보를 subsequent layer로 전달
    - resNet은 이러한 두 부분을 분리하였고, denseNet도 이러한 철학을 반영
    - **resNet의 경우는 각 layer마다 weight를 가지므로 모델이 무겁지만, denseNet의 경우는 각 layer에서 'collective knowledge(집단지성)'을 합치는 weight만 가지고, 이전의 정보를 그대로 보존하므로 상대적으로 가벼움**
    - 모델의 마지막 부분에서 모든 feature들을 고려하여 decision

- gradient의 흐름이 향상됨
    - 모든 layer가 gradient에 바로 접근 가능. 따라서, 모델이 깊어져도 학습이 가능
    - 즉, 더 쉽게 학습이 됨
    - regularizing 효과

-----

## 3. Related Work
- cascade structure
- Highway Networks
- ResNet
- GoogLeNet
- NIN, DSN, DFN

-----

## 4. DenseNet
# 4.1 resNet
- $$x_{l}$$ = $$H_{l}(x_{l-1}) + x_{l-1}$$
    - $$H_{l}(x)$$ = BN, ReLU, Conv로 구성
    - previous layer의 정보가 summation으로 새로운 정보에 더해지므로 정보의 흐름을 방해하게 됨

# 4.2 Dense connectivity
![dense](https://files.slack.com/files-pri/T1J7SCHU7-FA089DNN5/dense1.png?pub_secret=5217a542bf)
- $$x_{l}$$ = $$H_{l}([x_{0}, \cdots , x_{l-1}])$$
    - $$H_{l}(x)$$ = $$Conv_{3 \times 3}(ReLU(BN(x)))$$
    - 모든 previous layer의 feature map(output)을 concatenate해서 새로운 정보를 만들어줌
    - summation으로 연결되지 않으므로 정보가 섞이지 않음
    - 각 layer의 output이 subsequent layer의 input으로 들어가기 때문에 identity function이 없어도 정보가 보존이 됨

# 4.3 composite function
- $$H_{l}$$은 composite function
    - BN, ReLU, Conv의 합성함수

# 4.4 Pooling layers(Transition layer)
![dense2](https://files.slack.com/files-pri/T1J7SCHU7-FA0QD46JE/dense2.png?pub_secret=3272153d65)
- down sampling을 해주기 위해서 Network를 dense block으로 분리
    - feature map의 차원이 바뀌는 경우에는 down sampling을 해줘야 함
    - 중간에 down sampling을 해주는 layer를 Transition layer라고 함

- Transition layer
    - $$T(x)$$ = $$2X2-avgPooling(BN(Conv_{1 \times 1}(X)))$$
    - 각 dense block 사이에서 down sampling

# 4.5 Growth rate
- 각 $$H_{l}$$이 $$k$$개의 feature map을 만든다면, $$l$$-th layer의 input은 $$k_{0} + k \times (l-1)$$개
    - $$k$$ = growth rate
    - $$k_{0}$$ = input의 channel
 
- denseNet은 narrow layer
    - denseNet과 다른 모델의 가장 큰 차이점
    - k=12와 같이 아주 적은 수의 parameter만 사용해도 성능이 좋음

- 각 layer는 'collective knowledge'를 합치는 역할을 수행
    - 'collective knowledge'라고 불리는 이유는 각 layer에서의 정보를 모두 concatenation을 통해 합치기 때문
    - 모든 previous feature map을 concatenate하여 convolution하므로 정보를 합치는 역할
    - **$$k$$가 'growth rate'라고 불릴 수 있는 이유는 $$k$$의 크기를 제한하며 previous layer의 정보를 얼마나 많이 표현하는지를 제어할 수 있기 때문**

# 4.6 Bottleneck layers
- denseNet-B
    - $$H_{l}$$ = $$Conv_{3 \times 3}(ReLU(BN(Conv-{1 \times 1}(ReLU(BN(x))))))$$
    - output은 $$k$$개의 feature map 밖에 되지 않지만, 그에 비해 input은 굉장히 많은 편
    - 따라서, 먼저 1x1 convolution을 수행하여 feature map을 줄이는 방식으로 연산의 효율성을 높이는 방법을 사용 
    - 실험에서는, 1x1 convolution을 통해 $$4k$$개의 feature map으로 줄임

# 4.7 Compression
- denseNet-C
    - transition layer에서도 1x1 convolution을 통해 feature map의 수를 줄여줄 수 있음
    - dense block에서 총 $$m$$개의 feature map을 가지고 있다면, $$\theta m$$$개로 줄일 수 있음
    - $$\theta$$ = 1일 경우에는 feature map의 수가 변하지 않으며, $$\theta$$ < 1일 경우는 feature map의 수가 감소
    - $$\theta$$ < 1일 경우를 denseNet-C라고 명명
    - 실험에서는, $$\theta$$ = 0.5 사용

- denseNet-BC
    - - denseNet-C와 denseNet-C의 기법을 모두 사용했을 경우를 지칭

# 4.8 Implementation Details
![param](https://files.slack.com/files-pri/T1J7SCHU7-FA089EGUR/feature.png?pub_secret=e88fef874f)
- 보통 3개의 dense block으로 architecture 구성

-----

## 5. Experiment
# 5.1 Dataset
- CIFAR
- SVHN
- ImageNet

# 5.2 Training
- SGD
- learning rate = 0.1
- 30~60 epoch

# 5.3 Classification Results on CIFAR and SVHN
![result1](https://files.slack.com/files-pri/T1J7SCHU7-FA0MC2PMZ/result.png?pub_secret=2902936234)
- capacity
    - 모델의 크기가 커질수록 성능이 좋아진다.
    - resNet에서의 overfitting 등 최적화 문제가 없다.

- parameter efficiency
    - parameter가 더 적고, layer의 수가 더 적어도 다른 모델보다 훨씬 성능이 좋다.

- overfitting
    - parameter의 수가 적으므로 overfitting의 위험성이 낮다.

# 5.4 Classification Results on ImageNet
![result2](https://files.slack.com/files-pri/T1J7SCHU7-FA0QD5334/result2.png?pub_secret=b9b69ca266)

-----

## 6. Discussion
![result3](https://files.slack.com/files-pri/T1J7SCHU7-FA0UAQ9LM/result3.png?pub_secret=d609445f39)
- Model compactness
    - 모든 layer가 연결되어있고, feature들을 재사용하므로 굉장히 compact한 모델이다.

- Parameter efficiency
    - denseNet-BC가 denseNet 중에서 가장 성능이 좋다.
    - denseNet은 resNet보다 parameter가 더 적지만 성능이 좋다.

- Implicit Deep Supervision
    - denseNet의 layer는 모두 연결되어있기 때문에 gradient를 전달받을 수 있어서 각 layer에 classifier가 있는 효과를 보여준다.

- Stochastic vs. deterministic connection
    - resNet-1001의 성능이 denseBC-100 보다 더 좋은 성능을 보여준다.
    - stochastic depth regularization of residual network의 경우, layer간의 연결을 random하게 끊어줘서 주변의 layer와 direct connection을 만들어주는데, 이것을 반복하는 것은 denseNet의 연결과 비슷한 connectivity를 가지게 한다. 
    - 즉, random하게 여러 layer와 direct connection을 가지게 되므로 denseNet과 비슷한 효과를 가진다.

- Feature reuse
![result4](https://files.slack.com/files-pri/T1J7SCHU7-FA0UDPRAR/result4.png?pub_secret=3a847ca2c5)
- 각 dense block에서 layer의 weight가 적절히 분포
    - block 앞단의 feature역시 그 block내에서 직접적으로 활용된다는 것을 의미

- transition layer도 weight가 적절히 분포
    - Network가 전체적으로 정보의 전달이 잘되는 것을 의미

- 두 번째, 세 번째 densce block의 첫 번째 layer의 경우 weight가 거의 없음
    - denseNet-BC의 경우 정보가 잘 압축되어 있고, 정보를 잘 표현하고 있어서 그대로 유지시키는 것을 의미 

- classification layer의 weight가 비교적 중요도가 떨어져보이는 부분이 많아보임
    - 하지만, 이것은 classification layer의 경우 마지막으로 생성된 feature map에 집중하는 것을 의미

-----

## 7. Conclusion
- denseNet은 서로 다른 layer끼리 연결되어 있도록 구성된다.
- parameter의 수가 기존의 다른 모델보다 훨씬 적다.
- parameter의 수가 증가하더라도 성능이 저하되거나 overfitting이 발생하지는 않는다.
 
-----

## 8. Reference
- [https://arxiv.org/abs/1608.06993](https://arxiv.org/abs/1608.06993)
- [http://openresearch.ai/t/densenet-densely-connected-convolutional-networks/42](http://openresearch.ai/t/densenet-densely-connected-convolutional-networks/42)
