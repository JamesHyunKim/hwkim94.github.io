---
layout: post
title:  "AlexNet[1] ImageNet Classification with Deep Convolutional Neural Networks(2012) - Review"
date:   2018-05-21 02:00:00 +0900
categories: [deeplearning, cnn, alexnet, paperreview]
---

## 1. Abstract
- 2012 ILSVRC 우승작
- 최초로 딥러닝에 GPU를 통한 연산을 도입

-----

## 2. Introduction
- 더 좋은 성능을 보이기 위해서는 많은 데이터와, 강력한 모델이 필요
- CNN
    - 많은 image를 학습하기 위해서는 모델의 capacity가 커야하는데, 이를 CNN이 해결
    - width, height, depth를 조절하여 모델의 capacity를 결정
    - 더 적은 connection과 parameter를 가지고 있으므로 상대적으로 학습하기 쉬움
    - 주변 pixel의 정보도 같이 활요하므로 정보를 잘 담고있음

- GPU
    - 하지만, 여전히 CNN도 많은 연산량을 가지고 있음
    - GPU를 통해 해결

-----

## 3. DataSet
- ILSVRC-2010

-----

## 4. Architecture
# 4.1 ReLU Nonlinearity
### 4.1.1 Fault of Other Activation Functions
![activation](https://files.slack.com/files-pri/T1J7SCHU7-FAU2F09JT/activation.png?pub_secret=eeee77c407)
- sigmoid
    - $$\mid x \mid$$가 커질 경우, gradient가 0과 가까워지기 때문에, chain rule을 사용하는 gradient descent기법에서 gradient vanishing 문제가 발생할 위험이 크다.
    - gradient는 chain rule에 의하여 $$\frac{\partial L}{\partial w} = \frac{\partial L}{\partial \alpha} \times \frac{\partial \alpha}{\partial w}$$, $$\alpha = w^T x + b$$이기 때문에 input값에 영향을 받게 되는 특성이 있다. 또한, sigmoid의 결과값은 $$[0,1]$$범위에 존재하므로 다음 layer의 input값은 반드시 양수가 된다. 따라서 모든 parameter의 gradient값이 항상 같은 부호를 같게 되므로, gradient descent시에 지그재그로 움직이게 된다.

- tanh
    - 값이 $$[-1,1]$$로 saturated, 즉, 제한되어 있지만 sigmoid와 달리 음수값을 포함하며 zero-centered 되어있다. 

### 4.1.2 ReLu
![relu](https://files.slack.com/files-pri/T1J7SCHU7-FAS9ZJNSU/relu.png?pub_secret=1231c4a141)
- ReLU가 gradient descent 연산에서 다른 activation function보다 더 빠르게 학습된다.
    - sigmoid나 tanh처럼 값이 saturated 되어 있지 않고 linear하기 때문에 gradient가 사라지지 않는다.
    - exponential의 미분이 필요 없으므로 빠른 연산이 가능하다.

- zero-centered되어 있지 않다.

- $$x$$가 0보다 작을 때는 뉴런이 죽는다.  
    - 학습이 더 이상 진행되지 않는다는 단점이 존재
    - 그 자체로 dropout효과를 보일 수도 있다.

# 4.2 Training on Multiple GPUs
![gpu](https://files.slack.com/files-pri/T1J7SCHU7-FAU2HJP1V/gpu.png?pub_secret=dd29d40344)
- GPU 2개를 사용한 병렬학습
    - kernel의 절반씩 연산
    - 중간중간 GPU끼리 communicate하며, 모델을 섞어줌

- weight initialization에 상관없이 하나의 gpu는 color와 관련있는 정보, 하나의 gpu는 color와 무관한 정보를 학습하는 결과를 보여줌

# 4.3 Local Response Normalization
- ReLU에서는 입력값에 비례하여 출력값이 증가하게 되므로, 값을 중간에 normalization을 해주면 성능이 개선된다.
    - lateral inhibition, 즉, 강한 자극이 주변 약한 자극에 영향을 주는 효과를 방지

# 4.4 Overlapping Pooling
- max pooling 사용
- pooling을 겹치게 하는 방식으로 overfitting을 조금 방지해줌

# 4.5 Overall Architecture
![alexnet](https://files.slack.com/files-pri/T1J7SCHU7-FASTJ1HQB/alexnet.png?pub_secret=f928e13d3c)
- 5개의 convolution layer와 3개의 fully-connected layer로 구성 
- max pooling과 Local Response Normalization 같은 곳에서 사용
- 중간에서 서로 다른 GPU의 연산결과를 공유하게됨
- classification에 사용되는 모델이므로 마지막은 softmax 사용

-----

## 5. Reducing Overfitting
- parameter가 많은 모델이므로 overfitting의 위험이 높음

# 5.1 Data Augmentation
- overfitting을 방지하는 가장 쉬운 방법은 label을 유지하며 data set을 늘리는 transformation을 사용하는 것
    - translation과 reflection 사용
    - RGB의 intensity 변경

# 5.2 Dropout
- 일정 확률로 node를 학습시키지 않는 방식
    - 해당 parameter는 학습이 되지 않으므로 overfitting되지 않음

-----

## 6. Details of learning
- SGD
- weight initialization by zero-mean Gaussian distribution

-----

## 7. Results
![result](https://files.slack.com/files-pri/T1J7SCHU7-FASTL7F43/reult1.png?pub_secret=9639d8978f)

# 7.1 Qualitative Evaluations
![result](https://files.slack.com/files-pri/T1J7SCHU7-FASTLKWUB/result2010.png?pub_secret=345df18582)

-----

## 8. Discussion
- layer의 수가 줄어들면 성능이 나빠지므로 모델의 깊이가 딥러닝에서 중요한 부분이다.
- computing power가 좋아진 덕분에 모델의 크기를 늘려서 좋은 성능을 보일 수 있었다.

-----

## 9. Reference

- [https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)
- [https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/](https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/)
- [https://m.blog.naver.com/PostView.nhn?blogId=laonple&logNo=220654387455&proxyReferer=https%3A%2F%2Fwww.google.co.kr%2F](https://m.blog.naver.com/PostView.nhn?blogId=laonple&logNo=220654387455&proxyReferer=https%3A%2F%2Fwww.google.co.kr%2F)
- [http://nmhkahn.github.io/NN](http://nmhkahn.github.io/NN)
