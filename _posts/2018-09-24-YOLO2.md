---
layout: post
title:  "YOLO[2] YOLO9000: Better, Faster, Stronger(2016) - Review"
date:   2018-09-24 00:10:00 +0900
categories: [deeplearning, cnn, image-detection, yolo, paperreview]
---

## 1. Abstract
- YOLOv2
    - YOLO 모델을 개선
    - 빠르고, 정확성이 높음

- YOLO9000
    - joint training등의 기법을 통해 labelled detection data에 없던 object를 detect하고 더 많은 label을 예측 가능
    - 9000개의 class를 학습

-----

## 2. Introduction
- detection dataset의 한계
    - detection label을 만드는 것은 classification label보다 어려워서 detection data의 class수가 아주 적다는 문제가 존재

- classification data를 사용하여 현재의 detection system을 확장하는 방법을 제안
    - object를 계층적으로 분석하여 서로 다른 dataset을 통합하게 해줌

- joint training algorithm
    - object detector를 detection data와 classification data로 학습하게 만들어줌
    - detection data로 위치를 학습하고, classification data로 vocabulary 학습

-----

## 3. Better
![table2](https://files.slack.com/files-pri/T1J7SCHU7-FCYKQF3UZ/table2.png?pub_secret=80920650c6)
- YOLO의 단점
    - localization error가 큼
    - recall(얼마나 정확하게 맞는 label을 찾는지)가 작음 

- 모델의 성능을 높이기 위해서는 모델의 크기를 키우면 되지만, 이렇게 할 경우 속도가 느려지므로 YOLO는 network를 간단하게 만들어 representation을 잘 학습하게 하는 것을 목표로 함

# 3.1 Batch Normalization
- conv layer에 batch norm 추가
    - 빠르게 수렴하게 도와는 역할
    - 정규화(Regularization) 효과가 있어서 dropout을 해줄 필요가 없음

# 3.2 High Resolution Classifier
- YOLO의 경우 ImageNet의 224x224 image를 학습한 후, detection은 448x448 image에 대하여 수행
    - 갑자기 다른 해상도에 대해서 다시 학습하는 문제가 생김

- YOLOv2에서는 pre-trained CNN을 먼저 448x448 image에 대해서 다시 classification을 하도록 fine-tuning을 시키고 난 후에 detection을 학습
    - CNN이 더 높은 해상도에 적응할 시간을 주는 것

# 3.3 Convolutional With Anchor Boxes
- YOLO는 fully connected layer를 통해 직접 bounding box를 찾아내는 구조이며, class는 각 grid cell마다 예측
    - 만들어내는 bounding box의 수가 적고, 비슷한 장소에서 예측할 수 있는 class의 수에 제한이 생김

- YOLOv2에서는 Faster R-CNN처럼 anchor box를 미리 생성하고 offset 정보만 예측
    - fully connected layer 대신 1x1 convolution을 통해서 각 anchor box마다 class를 예측하는 방식을 사용
    - 각 anchor box마다 offset 정보, objectness, class 정보를 예측

- 추가적으로, YOLOv2에서는 pooling layer를 한 개 줄이고, 448x448 image를 416x416 image로 축소시켜 network를 모두 지난 후에 13x13 image가 되도록함
    - pooling layer를 줄이는 방식으로 해상도를 높임
    - 크기가 큰 object의 경우에 중심이 가운데에 위치하는 경우가 많으므로, size가 홀수가 되도록 조정하여 중심이 grid cell에 위치하게 함

# 3.4 Dimension Clusters
![fig2](https://files.slack.com/files-pri/T1J7SCHU7-FCZ15TJ4C/fig2.png?pub_secret=b0ef32f310)
- YOLOv2에서는 anchor box를 hand-picking하지 않고, k-means clustering을 통해 anchor box를 결정
    - distant function = $$d(box, centroid)$$ = $$1-IoU(box, centroid)$$ 
    - IoU를 distance 기준으로 잡게 되면 비슷한 모양과 크기를 가진 bounding box가 같은 cluster에 속하게 됨
    - cluster가 어떻게 생성되었는지를 보고, 각 클러스터를 기반으로 anchor box를 선택

- Faster R-CNN의 anchor box보다 적은 5개의 anchor box를 사용했음에도 불구하고, Avg IoU 값은 증가

# 3.5 Direct location prediction
![fig3](https://files.slack.com/files-pri/T1J7SCHU7-FCZCDQQSZ/fig3.png?pub_secret=80307f0724)
- 기존의 region proposal network에서는 anchor box를 사용할 경우에, anchor box의 (x,y) 좌표 때문에 안정성이 떨어짐
    - $$x_{new}$$ = $$\hat{x} * f_w(w) - x_{old}$$ 
    - $$y_{new}$$ = $$\hat{y} * f_h(h) - y_{old}$$ 
    - $$f_w$$, $$f_h$$의 범위가 정해져있지 않기 때문에 (x,y)좌표가 image의 어디로든 갈 수 있어서 parameter의 변화가 많은 학습초기의 안정성이 많이 떨어짐
    - region proposal은 제안된 위치에서 object를 찾으라는 것이기 때문에, 원래 위치에서 멀어지면 안되는데 위의 방법론은 원래 위치에서 멀어질 가능성이 높음

- YOLOv2에서는 YOLO에서 처럼 해당 grid cell내에서 anchor box의 위치를 찾는 방법을 사용
    - $$b_x$$ = $$\sigma(t_x) + c_x$$
    - $$b_y$$ = $$\sigma(t_y) + c_y$$
    - $$b_w$$ = $$p_w e^{t_{w}}$$
    - $$b_h$$ = $$p_h e^{t_{h}}$$
    - $$P(Object) \times IoU(b, Object)$$ = $$\sigma(t_o)$$
    - $$b$$=$$(b_x,b_y,b_w,b_h)$$ = ground truth box
    - $$p_w, p_h$$ = prior width and prior height
    - $$t_x, t_y, t_w, t_h, t_o$$ = predicted value
    - 즉, $$\sigma$$(sigmoid)를 사용하여 이동 범위를 0~1 사이로 제한하여 중심이 항상 추천된 grid cell 내부에 위치하도록 함
    - 따라서, 각 grid cell마다 5개의 bounding box를 예측하고, 각 bounding box마다 5개의 value를 예측함

# 3.6 Fine-Grained Features
- YOLOv2에서는 작은 object를 찾아내기 위하여 위하여 13x13x1024 size feature map과 바로 이전의 26x26x512 size feature map을 concatenate해줌
    - 26x26x512 size feature map을 위치를 고려(인접한 채널을 하나의 채널로 통합)하여 13x13x2048 size feature map으로 전환한 후 concatenate
    - 높은 해상도를 가진 26x26x512 size feature map은 작은 물체를 찾는 것에 도움이 됨
    - 높은 해상도와 낮은 해상도를 모두 이용하기 때문에 다양한 크기의 object를 모두 찾을 수 있음

# 3.7 Multi-Scale Training
![fig4](https://files.slack.com/files-pri/T1J7SCHU7-FCZ1ZP8DS/fig4.png?pub_secret=d70d024158)
- YOLOv2는 convolution과 pooling으로만 구성되어 있어서 input size의 영향을 받지않기 때문에, 학습시에 다양한 resolution의 image를 input으로 제공 가능
    - 이 방식으로 모델이 다양한 해상도에 대해서 모두 예측할 수 있도록 학습시킴
    - 매 10 batch마다 {320, 352, ..., 608} size 중에서 임의로 골라서 해당 size로 image를 resize
    - 낮은 해상도의 image에 대해서는 성능은 안좋지만 엄청 빠른 속도를 보여주며, 높은 해상도의 image에 대해서는 비교적 느리지만 좋은 성능을 보여줌  
    - 다양한 mode로 이용가능

# 3.8 Further Experiments
![table4-5](https://files.slack.com/files-pri/T1J7SCHU7-FCZ4GLVT5/table4.png?pub_secret=c40a691596)

-----

## 4. Faster
# 4.1 Darknet-19
![table6](https://files.slack.com/files-pri/T1J7SCHU7-FCZCS8CFP/table6.png?pub_secret=f30b204905)
- VGG 기반의 모델
    - 3x3 filter와 1x1 filter 사용
    - 19개의 convolutional layer와 5개의 pooling layer로 구성
    - 마지막에 global average pooling 사용

# 4.2 Training for classification
- ImageNet의 224x224 image를 모두 학습한 이후에 10epoch 동안 resized된 448x448 image를 다시 학습

# 4.3 Training for detection
- Darknet의 마지막 conv layer부분을 잘라내고 새로운 3x3-conv layer와 1x1-conv layer를 붙여줌 
    - 한 grid당 5개의 box가 각각 5개의 coordinate와 20개의 class에 대한 확률을 예측해야 하므로 125개의 filter가 필요

-----

## 5. Stronger
- detection dataset의 label은 대부분 'dog', 'boat'와 같은 general한 label이며, 그 수가 매우 적음
- 따라서, 이 논문에서는 더 많은 label에 대해서 학습하려는 시도를 함
    - classification dataset과 detection dataset을 섞어서 학습
    - backpropagation을 할 때, detection data는 YOLOv2의 모든 module을 학습시키지만 classification data는 classification-specific modole에 대해서만 학습을 시키는 방식

- label을 예측할 때 softmax를 사용할 수 있지만, 그냥 사용할 경우 COCO dataset의 'dog'와 ImageNet의 'Yorkshire terrier'가 배타적으로 예측이 되므로 문제가 발생
    - 계층적 구조를 이용하여 multi-label을 예측하도록 하는 방식을 사용

# 5.1 Hierarchical classification
![fig5](https://files.slack.com/files-pri/T1J7SCHU7-FD0JJJGQ6/fig5.png?pub_secret=64a7588ee3)
- ImageNet의 label은 WordNet이라는 language database을 따라서 만들어짐
    - WordNet은directed graph의 구조를 가짐
    - 예를 들어, 'dog'는 'canine'라는 class나 'domestic animal'라는 class 모두에 속할 수 있음
    - 따라서, full structure를 그대로 사용하지 않고, 가장 최단 거리의 class에만 속하게 하는 방식으로 tree로 변형하여 사용
    - tree구조가 되면 계층적 구조를 가지게 됨
    - root node = 'physical object'

- 만약 하위의 node가 label이면, 그 상위의 node도 같이 label로 선정
    - 예를 들어, 'Yorkshire terrier'이면 'dog'이고, 'dog'이면 'mammal'이고, 'mammal'이면 'animal'이며, 최종적으로 'physical object'에도 속함

- 같은 concept node에 속한 다른 class들만 사용하여 softmax를 계산하며, 전체 확률은 조건부확률을 이용하여 계산하는 방식 사용
    - 예를 들어, $$P(Yorkshire\; terrier)$$ = $$P(Yorkshire\; terrier \mid terrier)$$ $$\times$$ $$P(terrier \mid hunting \;dog)$$ $$\times \cdots\ times$$ $$P(mammal \mid animal)$$ $$\times$$ $$P(animal \mid physical\; object)$$ $$\times$$ $$P(physical\; object)$$

- backpropagation을 할 때에는, 실제 label node 이하의 node에 관한 parameter는 update하지 않음 

# 5.2 Dataset combination with WordTree
![fig6](https://files.slack.com/files-pri/T1J7SCHU7-FCYL76A9X/fig6.png?pub_secret=d134d33c6d)
- ImageNet과 COCO dataset을 섞어서 WordTree를 만들어냄

# 5.3 Joint classification and detection
![table7](https://files.slack.com/files-pri/T1J7SCHU7-FD0JJN2GN/table7.png?pub_secret=c99e96dca0)
- full ImageNet과 COCO를 섞어 약 9000개의 계층적 label을 만들어냄
- COCO data를 oversamping
- 3개의 anchor box 사용

-----

## 6. Conclusion
더 빠르고, 더 성능이 좋으며, 더 많은 class를 학습할 수 있다.

-----

## 7. Reference
- [https://arxiv.org/abs/1612.08242](https://arxiv.org/abs/1612.08242)
- [http://www.navisphere.net/6030/yolo9000-better-faster-stronger/](http://www.navisphere.net/6030/yolo9000-better-faster-stronger/)
- [http://www.modulabs.co.kr/?module=file&act=procFileDownload&file_srl=18093&sid=4047bac32b86972cf0f22158cf138685&module_srl=17958](http://www.modulabs.co.kr/?module=file&act=procFileDownload&file_srl=18093&sid=4047bac32b86972cf0f22158cf138685&module_srl=17958) 
- [https://m.blog.naver.com/sogangori/221011203855](https://m.blog.naver.com/sogangori/221011203855)
- [http://openresearch.ai/t/yolo9000-better-faster-stronger/21](http://openresearch.ai/t/yolo9000-better-faster-stronger/21)
- [https://www.youtube.com/watch?v=6fdclSGgeio](https://www.youtube.com/watch?v=6fdclSGgeio)
- [http://dhhwang89.tistory.com/136](http://dhhwang89.tistory.com/136)
